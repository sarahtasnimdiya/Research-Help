{"docstore/metadata": {"e1e21035-ad3e-4f3c-ab8d-b8b199657f37": {"doc_hash": "ff9b494f06ab2d9301cf95741443fa088208aac93ccbbf897ba21c90e3f8ad47"}, "16b16815-ddad-4841-882c-7c8729e2e9f0": {"doc_hash": "6f8eb09be4d6b57860976a87a017cff86b81a1abba1fe633f75051efe113c5be", "ref_doc_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37"}, "f1169cb7-640f-415b-9f84-a311732904fc": {"doc_hash": "a2615b26fd90c180d79a6c754dd9ddffad7fe40b3842befe1a42014562345c37", "ref_doc_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37"}, "a193c7e0-d519-4f16-8bf5-35d20b0ebb8b": {"doc_hash": "6b67c26d71a488780ebde37632d24ef4946b45fd627002a1713aa17e32bc65e6", "ref_doc_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37"}, "3c362945-6306-4823-b788-a94fd601dcd7": {"doc_hash": "4fcdd22f09e4d204bc36b16902056a505c116033900f46f07e5ba1dcebc61587", "ref_doc_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37"}, "eadc2d00-2987-4750-a112-6faa41dfbec6": {"doc_hash": "f9b7d0fa7f22f1db01989ae4d4046bc536bdde9b80f269e880e0773e324f3972", "ref_doc_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37"}, "35355a63-21da-4a02-9112-2db92eee72de": {"doc_hash": "48cc103938d679aef6a6865b8ade76bf50b1536e2ad2e996d914c9d014befdd1", "ref_doc_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37"}, "95c0f99c-38eb-4427-b3fa-f99237d54c81": {"doc_hash": "20ed570d624cf5960fb0e7867edd27fe9d8be01e468184aa5d90b51a7c36ae99", "ref_doc_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37"}, "489b5db9-868e-405c-a918-cf24ea66665d": {"doc_hash": "3941830c3292979a3aa758d21923a7f5f06f199c2aeba5d17032d9cd5ed9e48a", "ref_doc_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37"}, "3c626e82-4595-4ef4-9c8d-229ab15dad64": {"doc_hash": "3b3e9ebec86566b66a5b6058f5d50bf030a8dc79761da05e14e7c281657742cb", "ref_doc_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37"}, "b8f0b867-de85-41a7-a847-7efbd06a69b6": {"doc_hash": "9e40928933f334993d34399d0f52112242dcdef6262ca856c217489d037f63af", "ref_doc_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37"}, "54909c00-6b3d-4496-b69d-72110ea564e5": {"doc_hash": "e016ccb0eb0e2eabc84efeb355e5ea63ed875bb0489c7541201ce426944480d5", "ref_doc_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37"}, "db074777-b2b5-4192-87c6-7f3ee7c1b029": {"doc_hash": "261c58f325529d2ea936a4a5681aeac51afaadcebdad2af33f018962374c92f3", "ref_doc_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37"}, "888bede3-ab79-4dfe-8da7-19b862e6672a": {"doc_hash": "b7a7e293cc1cdd42e99650bf6dbae7dc1486aee45bccdfe15696eb23298c85ea", "ref_doc_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37"}, "a0925b83-532c-4cae-897d-04622286f332": {"doc_hash": "e1c8e30b39b7b64a79c1ced9f8be8960ee1a9736e44216a141f95141d517ab93", "ref_doc_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37"}, "ad3554ed-c72c-4d81-ab91-1c7e3ce95f82": {"doc_hash": "c0ee9847f3b1922a07468bcbcb45d682c15277e7b1d1995d7220d5d33941266c", "ref_doc_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37"}, "a45537ac-7c75-4531-93d4-50614f3ad27b": {"doc_hash": "5ab11c66cebaf79551ecdaade8810c782eecec5dde05978a1d3ada8f8270caad", "ref_doc_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37"}, "adf94ef1-93ab-40e8-9487-9bbe9f52a8ce": {"doc_hash": "5606c66bf02f109a4f70cec15ab023f6c0ec145aa5a635261f6d74c1930ddafd", "ref_doc_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37"}, "2f708ad4-353c-413c-b574-74f0eacb9fc6": {"doc_hash": "5cd17f4051250373bff077b181c5e5d8d37907612cba4632395d398d654d9f0e", "ref_doc_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37"}, "1d71f7c2-63ad-46c0-a68b-86a68fca7f25": {"doc_hash": "c3097cc25d65741e98c5b024a845d4f9ec1345dac5822fd16ba6f209a26944a3", "ref_doc_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37"}, "7ad8d4be-c2e0-44d7-aed5-575ec7d380f8": {"doc_hash": "414d942e826490651b175705b67e4268ac57a9a30e2c02372d8d36be72f7c77f", "ref_doc_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37"}, "c8b7adff-aa90-471e-9f23-82d6ce6372ec": {"doc_hash": "ce64d0c93c313d4b4985ebbd3b69865c5e89f2a76622f2dfd2bd7d4d63dc5cec", "ref_doc_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37"}, "3dfc22e8-725e-4b1a-a9fa-8a89553818da": {"doc_hash": "a52eaff48aaf41b5bb4b29d6d41336b702b16da7af05590cf7dc3ced480324b9", "ref_doc_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37"}}, "docstore/ref_doc_info": {"e1e21035-ad3e-4f3c-ab8d-b8b199657f37": {"node_ids": ["16b16815-ddad-4841-882c-7c8729e2e9f0", "f1169cb7-640f-415b-9f84-a311732904fc", "a193c7e0-d519-4f16-8bf5-35d20b0ebb8b", "3c362945-6306-4823-b788-a94fd601dcd7", "eadc2d00-2987-4750-a112-6faa41dfbec6", "35355a63-21da-4a02-9112-2db92eee72de", "95c0f99c-38eb-4427-b3fa-f99237d54c81", "489b5db9-868e-405c-a918-cf24ea66665d", "3c626e82-4595-4ef4-9c8d-229ab15dad64", "b8f0b867-de85-41a7-a847-7efbd06a69b6", "54909c00-6b3d-4496-b69d-72110ea564e5", "db074777-b2b5-4192-87c6-7f3ee7c1b029", "888bede3-ab79-4dfe-8da7-19b862e6672a", "a0925b83-532c-4cae-897d-04622286f332", "ad3554ed-c72c-4d81-ab91-1c7e3ce95f82", "a45537ac-7c75-4531-93d4-50614f3ad27b", "adf94ef1-93ab-40e8-9487-9bbe9f52a8ce", "2f708ad4-353c-413c-b574-74f0eacb9fc6", "1d71f7c2-63ad-46c0-a68b-86a68fca7f25", "7ad8d4be-c2e0-44d7-aed5-575ec7d380f8", "c8b7adff-aa90-471e-9f23-82d6ce6372ec", "3dfc22e8-725e-4b1a-a9fa-8a89553818da"], "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}}}, "docstore/data": {"16b16815-ddad-4841-882c-7c8729e2e9f0": {"__data__": {"id_": "16b16815-ddad-4841-882c-7c8729e2e9f0", "embedding": null, "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37", "node_type": "4", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "ff9b494f06ab2d9301cf95741443fa088208aac93ccbbf897ba21c90e3f8ad47", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1169cb7-640f-415b-9f84-a311732904fc", "node_type": "1", "metadata": {}, "hash": "1ee2067241720397b4ee9af5214c6d281bf3523810a2496fa5389ba7980960b1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Available online at www.ijournalse.org\n\nEmerging Science Journal\n\n(ISSN: 2610-9182)\n\nVol. x, No. x, xxxxxxxxx, 20xx\n\n\n\n\t\t\t\n\n\tEmerging Science Journal | Vol. x, No. x\n\n\tEmerging Science Journal | Vol. x, No. x\n\n\n\nDetecting Genuine Versus Fake Emotions: A Dual-Task Deep Learning Approach Using Facial Expression Analysis \n\n\n\nSarah Tasnim Diya 1, Most. Jannatul Ferdos2, Md Mizanur Rahman 3,Suman Ahmmed 4, Ohidujjaman5\n\n 1,2,3 Department of Computer Science & Engineering, Daffodil International University, Dhaka, Bangladesh\n\n4,5 Department of Computer Science & Engineering, United International University, Dhaka, Bangladesh\n\n\n\n\n\nAbstract\n\nFacial expression recognition (FER) is a relevant field of study with applications in human-computer interaction, healthcare, and security. Although recent approaches demonstrate excellent outcomes on the recognition of basic emotions, the authenticity of expressions (genuine versus fake) remains unexplored. In this work, we propose a dual-task deep learning framework based on EfficientNet-B0, enhanced with a lightweight squeeze-and-excitation (SE) attention mechanism, to collaboratively work on multiclass emotion recognition (seven categories: angry, disgust, fear, happy, neutral, sad and surprise) and authenticity classification (genuine vs fake). The architecture leverages a shared backbone for representing feature, followed by task-dedicated branches trained using categorical cross-entropy and focal loss, respectively. To overcome the lack of publicly available benchmarks incorporating authenticity labels, we designed a curated dataset annotated with both emotional categories and authenticity information. Experimental evaluation demonstrates that the proposed dual-task model with the SE attention mechanism achieves 98.5% accuracy for emotion recognition and 92.2% accuracy for authenticity prediction, emphasizing both the effectiveness of the framework and the inherent challenges of authenticity detection. Moreover, we present a deployable real-time system demonstrating the feasibility of integrating authenticity-aware FER into practical applications such as e-learning analytics, security surveillance, and affective computing.\n\n\n\n\n\n\n\nKeywords: Affective computing, authenticity detection, dual-task learning, EfficientNet, facial emotion, emotion recognition, attention mechanisms, deep learning.\n\n\n\n\n\nArticle History:\n\nReceived:\n\n\n\n\n\n\n\nRevised:\n\n\n\n\n\n\n\nAccepted:\n\n\n\n\n\n\n\nPublished:\n\n\n\n\n\n\n\n\n\n\n\n\n\n1- Introduction\n\nFacial emotion recognition (FER) has been a keystone in affective computing for decades with applications such as healthcare, education, surveillance, and human-computer interaction [1]. Conventional FER work mainly targets at recognizing the discrete emotions such as happiness, anger or sadness, achieving high classification accuracy on well-controlled data [2], [3]. But a significant challenge remains unaddressed: distinguishing if an expression is real or fake. Such an authenticity dimension is important in real-world applications, such as security screening, lie detection and trust evaluation in human\u2013AI communication.\n\n  While much of the existing FER methods have been developed focusing on emotion recognition alone, independent of authenticity, thereby overlooking the fact that two visually similar expressions of the same emotion may differ fundamentally in whether they are genuine or posed [4]. Even if the authenticity is taken into account, it is often conceived of as a separate binary problem, which is independent of emotion detecting [5]. This differentiation disregards the inherent association of the emotional category with actual and performed versions. As an example, minor micro-expressions that appear in actual anger are very different with exaggerated expressions that appear in simulated anger. This kind of interrelationship may be learned by employing a multi-task environment, which learns both shared facial patterns and task-related information at the same time.\n\n  In order to fill this gap, we develop a dual-task attention-enhanced model that recognizes emotions and detects authenticity at the same time. According to EfficientNet-B0, the model combines with lightweight attention layers where attention is paid to discriminative facial parts, and the multi-task structure allows sharing in learning between the two outputs. Experimental validation is conducted using a newly collected dataset of genuine and acted expressions over seven basic emotions.\n\n  The main contributions of this paper can be summarized as:\n\nA novel multitask architecture that simultaneously performs emotion recognition and authenticity detection in a single framework. \n\nIntroduction of an attention mechanism to enhance the differentiability of subtle emotional micro-patterns. \n\nA curated primary dataset with authenticity annotations, to enable a more fine-grained evaluation. \n\nIntegration of Explainable AI methods (LIME and Grad-CAM) to ensure the model transparency and comprehension of the decision made by the model.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5030, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "f1169cb7-640f-415b-9f84-a311732904fc": {"__data__": {"id_": "f1169cb7-640f-415b-9f84-a311732904fc", "embedding": null, "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37", "node_type": "4", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "ff9b494f06ab2d9301cf95741443fa088208aac93ccbbf897ba21c90e3f8ad47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "16b16815-ddad-4841-882c-7c8729e2e9f0", "node_type": "1", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "6f8eb09be4d6b57860976a87a017cff86b81a1abba1fe633f75051efe113c5be", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a193c7e0-d519-4f16-8bf5-35d20b0ebb8b", "node_type": "1", "metadata": {}, "hash": "973db094f00fd0e58058e4a6d3f4d0533060d84273115e3b4f4f6df528c14c41", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "In order to fill this gap, we develop a dual-task attention-enhanced model that recognizes emotions and detects authenticity at the same time. According to EfficientNet-B0, the model combines with lightweight attention layers where attention is paid to discriminative facial parts, and the multi-task structure allows sharing in learning between the two outputs. Experimental validation is conducted using a newly collected dataset of genuine and acted expressions over seven basic emotions.\n\n  The main contributions of this paper can be summarized as:\n\nA novel multitask architecture that simultaneously performs emotion recognition and authenticity detection in a single framework. \n\nIntroduction of an attention mechanism to enhance the differentiability of subtle emotional micro-patterns. \n\nA curated primary dataset with authenticity annotations, to enable a more fine-grained evaluation. \n\nIntegration of Explainable AI methods (LIME and Grad-CAM) to ensure the model transparency and comprehension of the decision made by the model. \n\nAn actual-world application pipeline demonstrating the feasibility with which this approach could be scaled.\n\n  The rest of this paper is organized as follows: We introduce related work in Section 2, the data and preprocessing are described in Section 3, the proposed model architecture is detailed in Section 4, experimental results and analysis are presented in Section 5, explainable AI are discussed starting from section 6; then we conclude with conclusion highlighting the future work in Section 7.\n\nRelated Work\n\nEmotion recognition and more specifically, differentiation of genuine from posed facial expressions has been a topic of interest in the domains of computer vision, affective computing and psychology for some time. The work on approaches ranges from psychological based, through cutting-edge deep learning systems, multimodal approaches and explainable AI. This section presents a brief review of the advancements that have been made by prior works in FER and genuine detection, in addition to uncovering a few gaps and justifying our work.\u00a0\n\nFormative Years of Emotion Authenticity \n\n\u00a0 The cognitive basis for separating real and the feigned emotions was established in Ekman\u2019s ground breaking research on universal facial expressions alongside Facial Action Coding System (FACS) [6]. Building on this, Jia et al. [7] also presented a systematic review of the datasets and methods with a focus on micro-expressions for authentic emotion. This work and other early works was the groundwork for the development of computational methods later used to detect authenticity.\u00a0\n\nCNN-Based Emotion Recognition\u00a0\n\n\u00a0 Recent developments in deep learning caused the convolutional neural networks (CNNs) to dominate FER. Bhagat et al. [8] applied CNNs on FER2013, achieving training accuracy of 82.56% and a very low verification accuracy of 65.68% because of class bias, overlapping expressions, and noise in images. Similarly, Ballesteros et al. [9] showed that image quality and context information significantly affect the recognition performance.\u00a0\n\n\u00a0 Hybrid solutions, such as integration of CNN and traditional features, have also been considered. Mathur and Gupta [10] incorporated Local Binary Patterns (LBP) with the Gabor filters to increase the performance on low resolution images. Anand and Babu [11] performed optimization of EfficientNetB0 using Red Fox Optimizer, which resulted in superior performance on FER2013 and EMOTIC. Manimohan et al. [12] utilized ResNet50 with Haar cascades for real-time recognition on webcams and also obtained competitive results.\u202f In another work, Khuntia & Kale [13] demonstrated that CRNN architecture enhances the temporal learning with an accuracy of 79.72% over FER2013+.\u00a0\n\nModel Architectures and Lightweight Designs Advancements\u00a0\n\n\u00a0 Towards higher performance and better efficiency, some advanced structures have been introduced as well. Islam et al. [14] used Neural Architecture Search (NASNet) combined with reinforcement learning, achieving more than 98% accuracy in FER2013 and CK+. Haider et al. [15], combining CNNs, SVM classifiers, and triplet loss, presented the best results on JAFFE and MMI datasets, but less generalization on AFFECTNET.\u00a0\n\n\u00a0 Light-weight and real-time models have also emerged. Singh [16] presented a land-mark based approach with 84.27% accuracy, which is sensitive to generic head rotation. Similarly, Parel et al. [17] designed CCR based OpenCV pipeline, achieving an accuracy of ~85% on real-world surveillance and retail.\u00a0\n\nVideo-Based and Temporal Approaches\u00a0\n\n\u00a0 Video FER is able to capture the temporal characteristics of expressions. Ashraf et al.", "mimetype": "text/plain", "start_char_idx": 3989, "end_char_idx": 8679, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a193c7e0-d519-4f16-8bf5-35d20b0ebb8b": {"__data__": {"id_": "a193c7e0-d519-4f16-8bf5-35d20b0ebb8b", "embedding": null, "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37", "node_type": "4", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "ff9b494f06ab2d9301cf95741443fa088208aac93ccbbf897ba21c90e3f8ad47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1169cb7-640f-415b-9f84-a311732904fc", "node_type": "1", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "a2615b26fd90c180d79a6c754dd9ddffad7fe40b3842befe1a42014562345c37", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c362945-6306-4823-b788-a94fd601dcd7", "node_type": "1", "metadata": {}, "hash": "c023498aa3b985b4d88aadbf1a4d9344fcb319033fdd2523beb59e844b029156", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Islam et al. [14] used Neural Architecture Search (NASNet) combined with reinforcement learning, achieving more than 98% accuracy in FER2013 and CK+. Haider et al. [15], combining CNNs, SVM classifiers, and triplet loss, presented the best results on JAFFE and MMI datasets, but less generalization on AFFECTNET.\u00a0\n\n\u00a0 Light-weight and real-time models have also emerged. Singh [16] presented a land-mark based approach with 84.27% accuracy, which is sensitive to generic head rotation. Similarly, Parel et al. [17] designed CCR based OpenCV pipeline, achieving an accuracy of ~85% on real-world surveillance and retail.\u00a0\n\nVideo-Based and Temporal Approaches\u00a0\n\n\u00a0 Video FER is able to capture the temporal characteristics of expressions. Ashraf et al. [18] employed a CNN on the ADFES-BIV collection and achieved an accuracy of 99.38%, but reported overfitting as an issue. Pruthviraja et al. [19] proposed FER2013+ to expand DCNNs and demonstrated the ethical issues of data collection, achieving an accuracy of 81.33%.\u00a0\n\n\u00a0 Explainability has also become important. Cardaioli et al. [20] utilized the SASE-FE dataset and explainable AI methods to discover facial movements of muscles which are important for authentication. Meanwhile, Miolla et al. [21] proposed PEDFE, a dataset for both dynamic genuine and posed expressions annotated by experts, which facilitated making more realistic databases.\u00a0\n\nAuthentic and Fake Emotion Detection\u00a0\n\n\u00a0 Recent studies have focused on authenticity detection, separating authentic from acted expressions. Annadurai et al. [22] introduced an Enhanced Boosted SVM with 98.08% results on SASE-FE as well as FED datasets. Sunil et al. [23] exploited temporal cues using modified CNNs and achieved 96% accuracy over the ChaLearn and Fake Smile Master datasets.\u00a0\n\n\u00a0 Fake emotion detection has also been advanced based on multimodal approaches. Arslan et al. [24] used ECG & GSR in VR environment, where + 97.78% accuracy is achieved. Jia et al. [25] projected emotion into Valence-Arousal-Dominance (VAD) space with the help of multiple modalities for cross-validation. Govea et al. [26] employed CNN-RNN models with reinforcement learning to personalise learning environments and achieved an accuracy of 88% over biometric, face, and speech data.\u00a0\n\n\u00a0Domain-Specific Applications\u00a0\n\n\u00a0 The detection of fake emotion is also applied in the field of education and health care. Evangeline, Parkavi [27] used InceptionV3 for online learning datasets and achieved 96.26% accuracy. Rathod et al. [28] combined social media, speech, and facial cues for remote MH monitoring and achieved accuracies of 92% (facial) and 87.96% (speech). Chethan and Vinay [29] developed a robust CNN-RNN model against occlusions and noise, while Barnwal and Barik [30] applied Haar-based classifiers for group emotion analysis.\u00a0\n\n\u00a0 Additional hybrid contributions have been reported, such as Zhang [31], fused ResNet and MobileNet (80% accuracy, but not good in class-wise performance), Singh et al. [32] have used real-time FER in behavioral learning analysis, and Ton-that & Cao [33] employed Combined Gray LBP (CGLBP) method using SVM, which has reached up to 99% over JAFFE and MUG datasets.\u00a0\n\nTable 1 provides a summary of recent and prominent past studies on emotion and authenticity recognition, starting with Jia et al. [7], which was the first article to propose the fundamental taxonomy of genuine-posed datasets and then the recent CNN, hybrid, and multimodal frameworks.\u00a0\n\nTable 1. Summary of Related Works on Facial Emotion and Authenticity Recognition\n\nRef.\u00a0\n\nYear\n\nMethod / Model\u00a0\n\nDataset Used\n\nKey Contribution\u00a0\n\nAcc. / Result\n\n[7] Jia et al.\u00a0\n\n2021\n\nReview of genuine vs. posed emotion databases\u00a0\n\nMultiple (e.g., CK+, MMI)\n\nSurveyed databases and methods for genuine vs. posed emotion detection", "mimetype": "text/plain", "start_char_idx": 7931, "end_char_idx": 11746, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3c362945-6306-4823-b788-a94fd601dcd7": {"__data__": {"id_": "3c362945-6306-4823-b788-a94fd601dcd7", "embedding": null, "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37", "node_type": "4", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "ff9b494f06ab2d9301cf95741443fa088208aac93ccbbf897ba21c90e3f8ad47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a193c7e0-d519-4f16-8bf5-35d20b0ebb8b", "node_type": "1", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "6b67c26d71a488780ebde37632d24ef4946b45fd627002a1713aa17e32bc65e6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eadc2d00-2987-4750-a112-6faa41dfbec6", "node_type": "1", "metadata": {}, "hash": "363c7246171534d476d3e79317c9a8930bcd7dd33fd38842111dd3712a36952d", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 1 provides a summary of recent and prominent past studies on emotion and authenticity recognition, starting with Jia et al. [7], which was the first article to propose the fundamental taxonomy of genuine-posed datasets and then the recent CNN, hybrid, and multimodal frameworks.\u00a0\n\nTable 1. Summary of Related Works on Facial Emotion and Authenticity Recognition\n\nRef.\u00a0\n\nYear\n\nMethod / Model\u00a0\n\nDataset Used\n\nKey Contribution\u00a0\n\nAcc. / Result\n\n[7] Jia et al.\u00a0\n\n2021\n\nReview of genuine vs. posed emotion databases\u00a0\n\nMultiple (e.g., CK+, MMI)\n\nSurveyed databases and methods for genuine vs. posed emotion detection\u00a0\n\n\n\n[8] Bhagat et al.\u00a0\n\n2024\n\nCNN\u00a0\n\nFER2013\n\nIdentified lass bias and low generalization in FER models\u00a0\n\n65.68% (val)\n\n[10] Mathur & Gupta\u00a0\n\n2024\n\nCNN + LBP + Gabor\u00a0\n\nCustom image set\n\nHybrid handcrafted\u2013deep features improved low-res emotion detection\u00a0\n\n90%\n\n[11] Anand & Babu\u00a0\n\n2024\n\nEfficientNet-B0 (optimized)\u00a0\n\nFER2013, EMOTIC\n\nMeta-heuristic optimization improved model generalization\u00a0\n\n96%\n\n[14] Islam et al.\u00a0\n\n2023\n\nNASNet (RL-based)\u00a0\n\nFER2013, CK+\n\nAuto-designed CNN achieved state-of-the-art FER accuracy\u00a0\n\n98%\n\n[15] Haider et al.\u00a0\n\n2023\n\nCNN + SVM + Triplet Loss\u00a0\n\nJAFFE, MMI\n\nMetric learning improved intra-class separability\u00a0\n\n95%\n\n[18] Ashraf et al.\u00a0\n\n2023\n\nVideo CNN\u00a0\n\nADFES-BIV\n\nModeled temporal cues in dynamic expressions\u00a0\n\n99.38%\n\n[20] Cardaioli et al.\u00a0\n\n2022\n\nExplainable CNN\u00a0\n\nSASE-FE\n\nApplied XAI to interpret micro-expressions for genuineness\u00a0\n\n96%\n\n[21] Miolla et al.\u00a0\n\n2022\n\nPEDFE dataset\u00a0\n\nPEDFE\n\nIntroduced dynamic benchmark labeled for genuine vs. posed\u00a0\n\n\n\n[22] Annadurai et al.\u00a0\n\n2022\n\nBoosted SVM\u00a0\n\nSASE-FE, FED\n\nEnhanced SVM classifier for real/fake emotion detection\u00a0\n\n98.08%\n\n[23] Sunil et al.\u00a0\n\n2023\n\nModified CNN\u00a0\n\nChaLearn, Fake Smile Master\n\nClassified real vs. fake emotions using temporal cues\u00a0\n\n96%\n\n[24] Arslan et al.\u00a0\n\n2024\n\nMultimodal (ECG + GSR)\u00a0\n\nCustom VR dataset\n\nBiosignal-based multimodal authenticity detection\u00a0\n\n97.78%\n\n[25] Jia et al.\u00a0\n\n2024\n\nMultimodal VAD\u00a0\n\nMultiple cross-domain\n\nModeled emotions in valence\u2013arousal\u2013dominance space", "mimetype": "text/plain", "start_char_idx": 11131, "end_char_idx": 13234, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "eadc2d00-2987-4750-a112-6faa41dfbec6": {"__data__": {"id_": "eadc2d00-2987-4750-a112-6faa41dfbec6", "embedding": null, "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37", "node_type": "4", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "ff9b494f06ab2d9301cf95741443fa088208aac93ccbbf897ba21c90e3f8ad47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c362945-6306-4823-b788-a94fd601dcd7", "node_type": "1", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "4fcdd22f09e4d204bc36b16902056a505c116033900f46f07e5ba1dcebc61587", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35355a63-21da-4a02-9112-2db92eee72de", "node_type": "1", "metadata": {}, "hash": "13a022b502164e6034b6c695eb62295012148d4d4c334c97c55f6a814a45a54f", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "[22] Annadurai et al.\u00a0\n\n2022\n\nBoosted SVM\u00a0\n\nSASE-FE, FED\n\nEnhanced SVM classifier for real/fake emotion detection\u00a0\n\n98.08%\n\n[23] Sunil et al.\u00a0\n\n2023\n\nModified CNN\u00a0\n\nChaLearn, Fake Smile Master\n\nClassified real vs. fake emotions using temporal cues\u00a0\n\n96%\n\n[24] Arslan et al.\u00a0\n\n2024\n\nMultimodal (ECG + GSR)\u00a0\n\nCustom VR dataset\n\nBiosignal-based multimodal authenticity detection\u00a0\n\n97.78%\n\n[25] Jia et al.\u00a0\n\n2024\n\nMultimodal VAD\u00a0\n\nMultiple cross-domain\n\nModeled emotions in valence\u2013arousal\u2013dominance space\u00a0\n\n\n\n  These recent works represent a milestone in distinguishing true and false emotion credibility based on deep learning, hybrid network and multimodal learning. There is, however, a large performance gap in real emotion recognition, partly because natural expressions are secretly known to be variant among individuals, and that they are culturally conditioned. The current detection systems, despite the improvements in accuracy, robustness, and adaptability, still possess long-standing challenges, particularly under the conditions of unrealistic and unbalanced datasets with an authenticity label, the difficulties in more subtle micro-expression reading, and the inability to generalize across domains of current models. To fill these gaps, more sustainable work is necessary to better diversify the data and work on the interpretability of the models and domain-adaptation strategies, which are to be used to create more scalable and reliable emotion-detecting systems.\n\nDataset Development\n\n  The development of a reliable dataset was crucial to the dual-task challenge to recognizing both primary emotions and its authenticity. Since there was no publicly available dataset that sufficiently captured the interrelation of emotion categories and fake vs. real facial expressions, we introduced a customized dataset for the purpose of this study.\u00a0\n\n\u00a0 The dataset includes seven basic emotional categories (Angry, Disgust, Fear, Happy, Neutral, Sad and Surprise), each of which was divided into 2 expressional classes: Genuine and Fake. This architecture allowed to learn both categorical emotion classification and authenticity analysis concurrently.\u00a0\n\nData Collection\u00a0\n\n\u00a0 Data were collected in a controlled setting and across diverse participants. The primary dataset consisted of 46 subjects (28 male, 18 female), including 7 professional actors to provide more plausible posed expressions. The sample consisted of individuals from a wide range of age, gender, color and geographic locations. Genuine emotions were evoked by natural inductions (e.g. engaging videos or personal recollections) and fake expressions were elicited with directed acting tasks.\u00a0\n\n\u00a0 Because it is difficult to obtain spontaneous emotion data, we made an attempt to create natural settings for genuine classes. Subjects were instructed to feel emotions as spontaneously as possible, while posed categories were exaggerated in order to reach the level of acted emotion.\u00a0\n\n\u00a0 All data collection and annotation procedures were performed under the supervision of a licensed psychologist for compliance with ethical regulations, to reduce possible discomfort among participants, and to confirm the genuineness of emotional responses. Written consent was taken from all the individuals before collecting the data.\u00a0\n\nData labeling and Annotation Protocol\u00a0\n\nAuthenticity labels were assigned under the supervision of a licensed psychologist following standardized facial expression criteria. To ensure labeling consistency, a subset of samples was independently re-evaluated at different intervals, yielding high agreement consistency. While formal multi-annotator Cohen\u2019s Kappa was not computed due to expert-guided single-annotator supervision, this protocol ensured reliable and psychologically grounded authenticity annotations. Micro-expressions and subtle cues were given specific consideration by the following guidelines:\u00a0\n\nEye involvement: The muscles around the eyes are involved in genuine emotions but may not be involved in fake emotions.\u00a0\n\nSymmetry: Natural emotions are a bit asymmetrical; pretended emotions are totally symmetrical.\u00a0\n\nConsistent Body-language: Authentic expressions coincide with form, posture and movement of lips\u00a0\n\nBrief micro-expressions: Genuine emotions are usually a fraction of a second long even repressed.\u00a0\n\n The detailed criteria used for each emotion class are summarized in Table 2.", "mimetype": "text/plain", "start_char_idx": 12733, "end_char_idx": 17142, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "35355a63-21da-4a02-9112-2db92eee72de": {"__data__": {"id_": "35355a63-21da-4a02-9112-2db92eee72de", "embedding": null, "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37", "node_type": "4", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "ff9b494f06ab2d9301cf95741443fa088208aac93ccbbf897ba21c90e3f8ad47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eadc2d00-2987-4750-a112-6faa41dfbec6", "node_type": "1", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "f9b7d0fa7f22f1db01989ae4d4046bc536bdde9b80f269e880e0773e324f3972", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "95c0f99c-38eb-4427-b3fa-f99237d54c81", "node_type": "1", "metadata": {}, "hash": "2e87bc161fbfcd703987fe63b3804f19da031279b2ac1db253ad61ae6b33dcf8", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 2. Labeling guide for authenticity across the seven basic emotion classes\n\nEmotion\u00a0\n\nGenuine Indicators\u00a0\n\nFake Indicators\u00a0\n\nHappy\u00a0\n\nEye wrinkles, natural cheek lift, symmetrical smile\u00a0\n\nMouth-only lift, asymmetrical or abrupt smile\u00a0\n\nSad\u00a0\n\nRaised inner brows, drooping eyelids, slow onset\u00a0\n\nNo drooping, mismatched eyes\u2013mouth, sudden onset\u00a0\n\nAngry\u00a0\n\nEyebrow tension, tight lips, clenched jaw\u00a0\n\nExaggerated brows, missing lip/jaw tension\u00a0\n\nFear\u00a0\n\nRaised eye brows, eyelids pressed tight together, brief open mouth\u00a0\n\nIncorrect brows, mismatched mouth, prolonged expression\u00a0\n\nSurprise\u00a0\n\nHigh brows, brief jaw drop\u00a0\n\nExcessive jaw drop, long-held, unsynced brows\u00a0\n\nDisgust\u00a0\n\nCurled nostrils, uplifted lip and squeezed eye lids\u00a0\n\nMouth curl only, missing wrinkles, exaggerated curl\u00a0\n\nNeutral\u00a0\n\nSubtle asymmetry, slight eye narrowing\u00a0\n\nForced symmetry, lips raised unnaturally long\u00a0\n\nCues are based on psychological micro-expression research and tested with the guidance of a licensed psychologist.\u00a0\n\nDataset Statistics\u00a0\n\n\u00a0 The overall the dataset included seven basic emotions (Angry, Disgust, Fear, Happy, Neutral Sad and Surprise) with two subsets for each of them (i.e. Genuine and Fake). The dataset structure consisted of a hierarchical folder arrangement, containing higher-level emotion classes and lower-level authenticity sub-folders.\u00a0\n\n\u00a0 In the first step, we obtained a total of 2224 raw samples for seven emotion categories (real or fake). After refinement, a sub-group of 1657 samples were used for the modeling in consideration of the class imbalance, noise and ambiguous cases. \n\nTo ensure the fairness of performance comparison, a dataset split was conducted, dividing 1325 samples (80%) into training set and 332 samples (20%) into test set.\u00a0Table 3 represents the distribution of the primary dataset across different catagories.\n\nTable 3. Final distribution of samples across emotion and authenticity categories\n\nEmotion\u00a0\n\nFake\u00a0\n\nGenuine\u00a0\n\nTotal\u00a0\n\nAngry\u00a0\n\n99\u00a0\n\n64\u00a0\n\n163\u00a0\n\nDisgust\u00a0\n\n152\u00a0\n\n115\u00a0\n\n267\u00a0\n\nFear\u00a0\n\n134\u00a0\n\n104\u00a0\n\n238\u00a0\n\nHappy\u00a0\n\n152\u00a0\n\n184\u00a0\n\n336\u00a0\n\nNeutral\u00a0\n\n141\u00a0\n\n169\u00a0\n\n310\u00a0\n\nSad\u00a0\n\n76\u00a0\n\n125\u00a0\n\n201\u00a0\n\nSurprise\u00a0\n\n101\u00a0\n\n41\u00a0\n\n142\u00a0\n\nTotal\u00a0\n\n855\u00a0\n\n802\u00a0\n\n1657\u00a0\n\n\u00a0A total of 1657 samples were selected from a pool of 2224 collected samples. The total number of images was 1657, with 1325 for training and 332 for testing.\u00a0\n\nPreprocessing and Augmentation\u00a0\n\n\u00a0 For training, images were resized to 224 \u00d7 224 pixels and preprocessed accordingly with input preprocessing functions consistent with the EfficientNet architecture family. Data augmentation (i.e., random horizontal flips, minor rotations, contrast and brightness changes) was used to generalize the model and reduce overfitting. These operations also helped to maintain natural features in facial expressions, and improved generalization. \n\nDespite SMOTE is conventionally applied to tabular data, in this work it was applied on learned feature embeddings extracted from the backbone network rather than on raw image pixels. This approach has been adopted in prior deep learning studies where embeddings form a structured feature space. Alternative strategies such as focal loss alone were also considered; however, empirical validation showed that combining focal loss with embedding-level SMOTE improved minority-class stability without introducing visual artifacts. The dataset preparation workflow, illustrated in Fig. 1, outlines the process from initial collection and refinement to final augmentation.\n\n\n\n\nFigure 1. Dataset preparation and augmentation workflow.\n\n Fig. 2 depicts representative samples of both authentic and counterfeit facial expressions of seven emotion categories, (Angry, Disgust, Fear, Happy, Neutral, Sad and Surprise demonstrating variations in facial muscle activation and movement dynamics observed to discriminate between authenticity labels during annotation.\n\n\u00a0Figure 2. Sample genuine and fake emotion examples across seven categories.\n\nMethodology\n\n  The proposed model aims to combine facial emotion recognition with authenticity detection into a single deep learning framework. Differing from traditional single-task model, our framework benefits from a joint feature backbone and task-specific branches which offer efficient learning of both categorical emotions and authenticity cues.\u00a0\n\nModel Architecture\u00a0\n\n  We utilized EfficientNet-B0 pretrained on ImageNet as the backbone encoder, due to its efficiency\u2013accuracy trade-off.", "mimetype": "text/plain", "start_char_idx": 17148, "end_char_idx": 21589, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "95c0f99c-38eb-4427-b3fa-f99237d54c81": {"__data__": {"id_": "95c0f99c-38eb-4427-b3fa-f99237d54c81", "embedding": null, "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37", "node_type": "4", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "ff9b494f06ab2d9301cf95741443fa088208aac93ccbbf897ba21c90e3f8ad47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "35355a63-21da-4a02-9112-2db92eee72de", "node_type": "1", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "48cc103938d679aef6a6865b8ade76bf50b1536e2ad2e996d914c9d014befdd1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "489b5db9-868e-405c-a918-cf24ea66665d", "node_type": "1", "metadata": {}, "hash": "64f8941fa8101c3e796b23a1d940c719580be26ae094d818780d0b2a860ee956", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Figure 1. Dataset preparation and augmentation workflow.\n\n Fig. 2 depicts representative samples of both authentic and counterfeit facial expressions of seven emotion categories, (Angry, Disgust, Fear, Happy, Neutral, Sad and Surprise demonstrating variations in facial muscle activation and movement dynamics observed to discriminate between authenticity labels during annotation.\n\n\u00a0Figure 2. Sample genuine and fake emotion examples across seven categories.\n\nMethodology\n\n  The proposed model aims to combine facial emotion recognition with authenticity detection into a single deep learning framework. Differing from traditional single-task model, our framework benefits from a joint feature backbone and task-specific branches which offer efficient learning of both categorical emotions and authenticity cues.\u00a0\n\nModel Architecture\u00a0\n\n  We utilized EfficientNet-B0 pretrained on ImageNet as the backbone encoder, due to its efficiency\u2013accuracy trade-off. The normalised 224\u00d7224\u00d73 input images were passed through the encoder to generate condensed embeddings using global average pooling. These embeddings were then fed through a couple of task-specific branches.\u00a0\n\n  The emotion recognition branch had a fully connected layer projecting to seven output units with softmax, generating probabilities over the emotion categories (angry, disgust, fear,happy, neutral, sad, surprise).\u00a0\u00a0\n\n  The authenticity branch, on the other hand, contained a fully connected layer that mapped to a single output neuron with sigmoid activation, to differentiate real and fake expressions. Dropout layers were further used in the task-specific branches to mitigate overfitting during training. Moreover, a lightweight attention module named squeeze-and-excitation (SE) was embedded into the backbone to improve discriminative capacity by focusing on important regions of faces such as eyes, brows, and mouth. Additionally, stochastic depth regularization was incorporated within the backbone to improve generalization. The squeeze-and-excitation (SE) attention modules are embedded within the MBConv blocks of the EfficientNet-B0 backbone, following depthwise convolution and before residual connections. This placement enables channel-wise recalibration of intermediate feature maps, emphasizing discriminative facial regions relevant to both emotion and authenticity tasks.\u00a0\u00a0\n\nThe proposed architecture assumes a meaningful interdependence between facial emotion categories and expression authenticity, which motivates joint feature learning through a shared backbone. The interdependence between emotion category and authenticity was empirically examined through ablation experiments. When the authenticity branch was removed, emotion classification performance declined, particularly for subtle expressions such as fear and sadness. Conversely, authenticity prediction accuracy deteriorated when emotion supervision was excluded. These findings indicate that shared feature representations capture complementary cues across both tasks, supporting the assumption that emotion category and authenticity are jointly informative rather than independent learning objectives.\n\nThe overall workflow of the proposed dual-task model, including shared feature extraction and the two parallel output branches, is illustrated in Fig. 3, which outlines the architecture\u2019s hierarchical flow from preprocessing to final classification outputs.\n\n Figure 3. Proposed dual-task architecture using EfficientNet-B0 with SE attention.\n\nTraining Procedure\u00a0\n\n\u00a0 To mitigate the distributional imbalance in authentic labels, SMOTE-based oversampling was used. Training was done using Augmentation methods such as horizontal flips, random rotations or brightness changes to enhance robustness and avoid overfitting.\u00a0\n\n\u00a0 The training loss function integrated the following two components, categorical cross-entropy for emotion recognition and BCE for authenticity classification.\n\n\t\n\n\t(1)\n\n\u00a0 where we fixed the weights as \u03b1 = 0.4 and \u03b2 = 0.6 to emphasize the authenticity task. We optimized our model using the Adam optimizer with learning rate of 1 \u00d710\u207b\u2074 and ReduceLROnPlateau scheduler. Training was done for maximal 100 epochs with early stopping (patience = 10).\u00a0\n\n\u00a0 The main hyperparameters and training setup are presented in Table 4.\u00a0\n\nTable 4. Training hyperparameters and model configuration.\n\n Parameter\u00a0\n\nValue\u00a0\n\nBackbone\u00a0\n\nEfficientNet-B0\u00a0\n\nInput Size\u00a0\n\n224 \u00d7 224 \u00d7 3\u00a0\n\nBatch Size\u00a0\n\n32\u00a0\n\nOptimizer\u00a0\n\nAdam\u00a0\n\nInitial LR\u00a0\n\n1e-4\u00a0\n\nLR Scheduler\u00a0\n\nReduceLROnPlateau\u00a0\n\nEpochs\u00a0\n\n100 (early stopping=10)\u00a0\n\nLoss Functions\u00a0\n\nCross-Entropy, BCE\u00a0\n\nLoss Weights\u00a0\n\n\u03b1 = 0.4, \u03b2 = 0.6\u00a0\n\n\u00a0\u03b1 and \u03b2 denote weighting factors applied to balance the emotion recognition and authenticity classification losses, respectively.\u00a0Multiple weighting combinations were initially evaluated during preliminary experiments. Balanced settings (\u03b1 = \u03b2 = 0.5) resulted in reduced authenticity sensitivity, while higher emotion weighting led to degraded genuineness detection.", "mimetype": "text/plain", "start_char_idx": 20633, "end_char_idx": 25640, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "489b5db9-868e-405c-a918-cf24ea66665d": {"__data__": {"id_": "489b5db9-868e-405c-a918-cf24ea66665d", "embedding": null, "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37", "node_type": "4", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "ff9b494f06ab2d9301cf95741443fa088208aac93ccbbf897ba21c90e3f8ad47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "95c0f99c-38eb-4427-b3fa-f99237d54c81", "node_type": "1", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "20ed570d624cf5960fb0e7867edd27fe9d8be01e468184aa5d90b51a7c36ae99", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c626e82-4595-4ef4-9c8d-229ab15dad64", "node_type": "1", "metadata": {}, "hash": "ad05042d5fde0252e40adaf4acdb0650b90eab9fc6345c91822dad0efaa84ff9", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Table 4. Training hyperparameters and model configuration.\n\n Parameter\u00a0\n\nValue\u00a0\n\nBackbone\u00a0\n\nEfficientNet-B0\u00a0\n\nInput Size\u00a0\n\n224 \u00d7 224 \u00d7 3\u00a0\n\nBatch Size\u00a0\n\n32\u00a0\n\nOptimizer\u00a0\n\nAdam\u00a0\n\nInitial LR\u00a0\n\n1e-4\u00a0\n\nLR Scheduler\u00a0\n\nReduceLROnPlateau\u00a0\n\nEpochs\u00a0\n\n100 (early stopping=10)\u00a0\n\nLoss Functions\u00a0\n\nCross-Entropy, BCE\u00a0\n\nLoss Weights\u00a0\n\n\u03b1 = 0.4, \u03b2 = 0.6\u00a0\n\n\u00a0\u03b1 and \u03b2 denote weighting factors applied to balance the emotion recognition and authenticity classification losses, respectively.\u00a0Multiple weighting combinations were initially evaluated during preliminary experiments. Balanced settings (\u03b1 = \u03b2 = 0.5) resulted in reduced authenticity sensitivity, while higher emotion weighting led to degraded genuineness detection. The selected values (\u03b1 = 0.4, \u03b2 = 0.6) provided the best trade-off between stable emotion classification and improved authenticity discrimination, and performance was observed to be relatively stable within a \u00b10.1 range.\n\nThe entire experimental procedure of the suggested dual-task framework, as well as data preparation, preprocessing, model building, training-testing, and evaluation, is outlined in Fig. 4, which shows the sequential pipeline starting with the data acquisition and culminating in the final emotion and authenticity prediction.\n\nFigure 4. Methodology pipeline of the proposed dual-task model. Deployment Pipeline.\n\nDeployment Pipeline\u00a0\n\n\u00a0The emotion and genuineness recognition dual-task model employs an EfficientNet-B0 backbone with squeeze-and-excitation (SE) attention layer. It has dual-outputs: (i) a softmax head for seven basic emotions (Angry, Disgust, Fear, Happy, Sad, Surprise and Neutral), and (ii) a sigmoid activation head for the authenticity label (authentic vs. fake). The best checkpoint (. pth) was preserved for inference.\u00a0\n\n\u00a0The backend, built using FastAPI, has face detection based on the MediaPipe framework by Google. The detected faces are resized to 224\u00d7224 pixels, and normalized before being fed to the network as a tensor. The predicted probabilities of emotions and authenticity are produced by the model, with an authenticity score thresholded at 0.5. Output bounding boxes and predictions are streamed to the frontend using WebSockets for real time on demand frame analysis.\u00a0\n\n\u00a0The frontend written in HTML5 and JavaScript receives live video and enables the submission of frames manually to be analysed. The interface has the 3-step sequence, rather than the streaming, which involves Starting the camera, connecting with the server, and Test Frame. Visualization of bounding boxes with predicted emotions and authenticity labels is superimposed and frames can be saved to be examined qualitatively.\u00a0\n\nThe system is deployed directly onto Render without being containerized. It is based on FastAPI and Uvicorn servers and CORS middleware to provide a secure means of communication. The entire codebase is broken down into individual modules (e.g. model.py, app.py, index.html) with a requirements.txt containing the list of dependencies needed to ensure seamless deployment across cloud environments.\u00a0\n\nIn Fig 5. A real-time deployment pipeline is presented, illustrating the sequential flow of live frame capture, backend processing, model inference, and frontend visualization.\n\nFigure 5. Deployment pipeline for real-time inference.\n\nComparative Model Architectures\u00a0\n\n\u00a0 In order to guarantee reliability and fairness of evaluation, a series of baseline and hybrid models were created with the same preprocessing, training, and testing conditions. The models were initialized with 224 x 224 x 3 normalized face inputs and trained with Adam optimizer (learning rate = 1 \u00d7 10\u207b\u2074) in its joint loss configuration, which matched the proposed framework. The following architectures were taken to have comparative evaluation:\u00a0\n\nCNN (Baseline):\u00a0\n\nA lightweight convoluted neural network that is built as a benchmark of dual-task learning. The model is made up of three convolutional blocks and each block comprises a convolutional layer of 3 x 3 with ReLU activation and a max-pooling layer to do spatial downsampling. The resulting feature maps are flattened and fed through a single fully connected 128-neuron fully connected layer with a dropout rate of 0. 5 to regularize them. Two parallel output heads are then implemented namely: a seven-unit softmax layer which identifies emotion and a unit sigmoid layer which identifies authenticity.", "mimetype": "text/plain", "start_char_idx": 24935, "end_char_idx": 29334, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3c626e82-4595-4ef4-9c8d-229ab15dad64": {"__data__": {"id_": "3c626e82-4595-4ef4-9c8d-229ab15dad64", "embedding": null, "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37", "node_type": "4", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "ff9b494f06ab2d9301cf95741443fa088208aac93ccbbf897ba21c90e3f8ad47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "489b5db9-868e-405c-a918-cf24ea66665d", "node_type": "1", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "3941830c3292979a3aa758d21923a7f5f06f199c2aeba5d17032d9cd5ed9e48a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b8f0b867-de85-41a7-a847-7efbd06a69b6", "node_type": "1", "metadata": {}, "hash": "da0f85a2665826c1850a6f1aeb08e116f83ba7fb0d2b08054e67b52ff4205c03", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "The models were initialized with 224 x 224 x 3 normalized face inputs and trained with Adam optimizer (learning rate = 1 \u00d7 10\u207b\u2074) in its joint loss configuration, which matched the proposed framework. The following architectures were taken to have comparative evaluation:\u00a0\n\nCNN (Baseline):\u00a0\n\nA lightweight convoluted neural network that is built as a benchmark of dual-task learning. The model is made up of three convolutional blocks and each block comprises a convolutional layer of 3 x 3 with ReLU activation and a max-pooling layer to do spatial downsampling. The resulting feature maps are flattened and fed through a single fully connected 128-neuron fully connected layer with a dropout rate of 0. 5 to regularize them. Two parallel output heads are then implemented namely: a seven-unit softmax layer which identifies emotion and a unit sigmoid layer which identifies authenticity. The model is trained at once on both tasks on categorical cross-entropy and binary cross-entropy loss, with a total number of trainable parameters approximate to 3.3 million.\u00a0\n\nThe schematic structure of this baseline model is illustrated in Fig. 6, highlighting its simple convolutional design and dual-output configuration.\n\nFigure 6. Baseline CNN model architecture for dual-task classification.\n\nEfficientNet-B0 + MobileNetV2 (Hybrid Branch):\u00a0\n\nA two-stream hybrid network with EfficientNet-B0 to identify emotions and MobileNetV2 to detect authenticity. Normalized 224 x 224 x 3 inputs were processed by each branch to generate global pooled embeddings which were then fed through dense layers (256 units in the case of emotion, 128-64 units in the case of authenticity) with ReLU activations and dropout regularization. The model was optimized with a combination of the categorical cross-entropy and binary focal loss (\u03b3 = 2), with loss weights of 1.0 and 1.5 respectively. Around 6.8 million parameters were applied, of which 4.33 million were trainable. This design evaluated the effectiveness of separate feature encoders to each subtask.\u00a0The architecture is depicted in Fig. 7, which outlines the parallel feature encoding and distinct dense layers for each task.\n\nFigure 7.  Hybrid dual-branch EfficientNet-B0 + MobileNetV2 architecture.\n\nEfficientNet-B3 (Feature Embedding + Dense Classifier):\u00a0\n\nA configuration with featured embedding, where a pretrained EfficientNet-B3 backbone (frozen by the time of training) was taken, as a high-level feature extractor. Each image was converted to 224 x 224 x 3 and normalized and fed through the backbone to generate 1536 dimensional global pooled embeddings. This was then fed to a dense layer of 256 neurons which had ReLU activation and a dropout rate of 0.3 to be regularized. Two output heads were connected independently a seven-unit softmax classifier used to identify emotions and a one-unit sigmoid neuron used to predict authenticity. To reduce the imbalance in the classes, Synthetic Minority Oversampling Technique (SMOTE) was utilized and the emotion labels were matched to oversampled embeddings via nearest-neighbor mapping. The model was trained on Adam (learning rate = 1 \u00d7 10\u207b\u2074) with categorical and binary cross-entropy loss and evaluated in terms of accuracy and AUC. This system has compared generalization ability of frozen EfficientNet feature representation with shallow dense classifiers on dual-task learning.\u00a0An overview of the embedding-based architecture is shown in Fig.8, emphasizing the frozen feature extraction and shallow classifier setup.\n\nFigure 8.   EfficientNet-B3 embedding-based architecture.\n\nMobileNetV2 + Xception (Embedding Fusion):\u00a0\n\nA multimodel embedding fusion framework integrating feature representations from two pretrained convolutional backbones; MobileNetV2 and Xception, both initialized on ImageNet and frozen during training. Each 224 \u00d7 224 \u00d7 3 input was processed through both encoders to generate 1280-dimensional (MobileNetV2) and 2048-dimensional (Xception) global average pooled embeddings. These were concatenated to form a unified 3328-dimensional representation capturing both lightweight and deep spatial hierarchies. The fused embedding was passed through a dense layer of 256 neurons with ReLU activation and a dropout rate of 0.3 to prevent overfitting. Two output layers followed: a seven-class softmax head for emotion recognition and a single-unit sigmoid head for authenticity detection. SMOTE-based resampling was applied to balance authenticity labels, and emotion labels were aligned via index matching.", "mimetype": "text/plain", "start_char_idx": 28446, "end_char_idx": 32968, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "b8f0b867-de85-41a7-a847-7efbd06a69b6": {"__data__": {"id_": "b8f0b867-de85-41a7-a847-7efbd06a69b6", "embedding": null, "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37", "node_type": "4", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "ff9b494f06ab2d9301cf95741443fa088208aac93ccbbf897ba21c90e3f8ad47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c626e82-4595-4ef4-9c8d-229ab15dad64", "node_type": "1", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "3b3e9ebec86566b66a5b6058f5d50bf030a8dc79761da05e14e7c281657742cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "54909c00-6b3d-4496-b69d-72110ea564e5", "node_type": "1", "metadata": {}, "hash": "947d75258f34f07bc15c2c4faffb7649ef040abdad25162e2d517c65adf6c6d7", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "MobileNetV2 + Xception (Embedding Fusion):\u00a0\n\nA multimodel embedding fusion framework integrating feature representations from two pretrained convolutional backbones; MobileNetV2 and Xception, both initialized on ImageNet and frozen during training. Each 224 \u00d7 224 \u00d7 3 input was processed through both encoders to generate 1280-dimensional (MobileNetV2) and 2048-dimensional (Xception) global average pooled embeddings. These were concatenated to form a unified 3328-dimensional representation capturing both lightweight and deep spatial hierarchies. The fused embedding was passed through a dense layer of 256 neurons with ReLU activation and a dropout rate of 0.3 to prevent overfitting. Two output layers followed: a seven-class softmax head for emotion recognition and a single-unit sigmoid head for authenticity detection. SMOTE-based resampling was applied to balance authenticity labels, and emotion labels were aligned via index matching. The model was trained using the Adam optimizer (learning rate = 1 \u00d7 10\u207b\u2074) was used to train the model on categorical and binary cross-entropy losses, and the model performance was evaluated using accuracy and AUC statistics. The objective of this setup was to investigate the effect of cross-model feature fusion on the inter-class discrimination in the domains of emotion and authenticity.\u00a0This configuration is visually described in Fig. 9, illustrating the parallel feature fusion and dense integration layers.\n\nFigure 9.  MobileNetV2 + Xception embedding fusion architecture.\n\nEfficientNet-B0 (Dual-Branch Without SE):\u00a0\n\nAn efficient two-task framework, which utilizes EfficientNet-B0 as a common backbone to conduct emotion recognition and authenticity prediction at the same time. To obtain shared embeddings, the 224 x 224 x 3 inputs were fed through the pretrained EfficientNet-B0 encoder, which used global average pooling, to obtain shared embeddings. Two task heads were connected two dense-batch-norm-dropout sequence (256 units) with softmax activation, which used emotion classification, and a more profound convolution-dense pipeline with sigmoid activation, which used authenticity checking. Training of the network was performed with joint optimization of categorical cross-entropy (emotion) and binary focal loss (\u03b3 = 1.5) with a loss weight of 1.0:1.5. The overall number of parameters was about 11.6 million (11.5 million trainable). This model was used as the ablation baseline to evaluate the effects of SE attention as well as stochastic-depth regularization in the proposed design.\u00a0The network layout is depicted in Fig. 10, showing the dual-branch design before integrating SE attention.\n\n\n\nFigure 10.   EfficientNet-B0 dual-branch model without SE attention.\n\nThe suggested dual-task EfficientNet-B0 and SE attention (as described in the previous 4-1) has been used as the baseline configuration to benchmark all the comparative models. All networks were trained and tested with the same set of curated data through stratified sampling to maintain an equal amount of each classification.\u00a0\n\nTo ensure fairness, all baseline models were evaluated under comparable training conditions. When pre-trained backbones were used, both frozen and fine-tuned variants were tested, and the best-performing configuration was reported. The proposed model was trained end-to-end under the same optimization constraints, ensuring that performance gains stem from architectural design rather than training advantages.\n\nExperimental Results and Discussion\n\n  The multi-task framework was meticulously tested on the curated dataset to measure its success in identification of both category-dependent emotions and authenticity cues. The performance was evaluated by accuracy, precision, recall, F1-score, Cohen\u2019s Kappa, Matthews Correlation Coefficient (MCC), and AUC. Bootstrapped confidence intervals were presented where appropriate to verify statistical significance.\n\nEvaluation Metrics\u00a0\n\n\u00a0In order to determine the quantitative effectiveness of the emotion recognition and authenticity detection problems, a number of standard evaluation measures were used. These are accuracy, precision, recall, F1-score, Cohen Kappa, Matthews Correlation Coefficient (MCC), and the Area Under the ROC Curve (AUC). The estimation of bootstrapped confidence intervals was also done to provide statistical robustness. These measures are defined as follows:\u00a0\n\nAccuracy\u00a0\n\n\u00a0 Accuracy is a percentage of the total correct predictions of all the instances considered. It represents the general performance of the model both in emotion and authenticity tasks. Mathematically it can be represented as:\u00a0\n\n\t\n\n\t(2)\n\n\u00a0\u00a0 A higher accuracy indicates better overall model correctness; however, it may not fully capture model reliability in imbalanced datasets.", "mimetype": "text/plain", "start_char_idx": 32023, "end_char_idx": 36810, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "54909c00-6b3d-4496-b69d-72110ea564e5": {"__data__": {"id_": "54909c00-6b3d-4496-b69d-72110ea564e5", "embedding": null, "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37", "node_type": "4", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "ff9b494f06ab2d9301cf95741443fa088208aac93ccbbf897ba21c90e3f8ad47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b8f0b867-de85-41a7-a847-7efbd06a69b6", "node_type": "1", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "9e40928933f334993d34399d0f52112242dcdef6262ca856c217489d037f63af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "db074777-b2b5-4192-87c6-7f3ee7c1b029", "node_type": "1", "metadata": {}, "hash": "dfff523f971c24397656a70e079aacd9b008b2a943a17d98252f87ae5527743e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Bootstrapped confidence intervals were presented where appropriate to verify statistical significance.\n\nEvaluation Metrics\u00a0\n\n\u00a0In order to determine the quantitative effectiveness of the emotion recognition and authenticity detection problems, a number of standard evaluation measures were used. These are accuracy, precision, recall, F1-score, Cohen Kappa, Matthews Correlation Coefficient (MCC), and the Area Under the ROC Curve (AUC). The estimation of bootstrapped confidence intervals was also done to provide statistical robustness. These measures are defined as follows:\u00a0\n\nAccuracy\u00a0\n\n\u00a0 Accuracy is a percentage of the total correct predictions of all the instances considered. It represents the general performance of the model both in emotion and authenticity tasks. Mathematically it can be represented as:\u00a0\n\n\t\n\n\t(2)\n\n\u00a0\u00a0 A higher accuracy indicates better overall model correctness; however, it may not fully capture model reliability in imbalanced datasets.\u00a0\n\nPrecision\u00a0\n\n\u00a0 Precision (also known as Positive Predictive Value) measures the fraction of correctly identified positive instances among all predicted positives. In authenticity detection, this measures the rate of the falsely identified fake and genuine samples.\u00a0\n\n\t\n\n\t(3)\n\n\u00a0\u00a0 High precision means that there will be a lesser false-positive rate, and that the model is capable of not mistaking one category of data as another one.\u00a0\n\nRecall (Sensitivity)\u00a0\n\n\u00a0 Recall is a measure of the capacity of the model to accurately classify all the relevant cases of a particular type. For example, in emotion recognition, it shows the proportion of true happy or angry samples that are correctly identified.\u00a0\n\n\t\n\n\t(4)\n\n\u00a0\u00a0 High value of recall implies that the model does not misleadingly detect real positives but does not account for false alarms.\u00a0\n\nF1-Score\u00a0\n\n\u00a0 The F1-Score provides a harmonic mean between precision and recall, offering a balanced measure that is particularly meaningful when dealing with class imbalance, as in the genuine vs. fake subtask.\u00a0\n\n\t\n\n\t(5)\n\n\u00a0\u00a0 A high F1-Score means that classification is consistent and stable across the categories.\u00a0\n\nCohen\u2019s Kappa (\u03ba)\u00a0\n\n\u00a0 Cohen's Kappa measures inter-class agreement between predicted and actual labels, adjusted for chance agreement. It gives a more dependable and more conservative assessment of performance than accuracy.\n\n\t\n\n\t(6) \n\nWhere  denotes the observed agreement and  the expected agreement by chance. A value of more than 0.80 implies high reliability and a value between 0.60-0.79 implies a high degree of agreement.\u00a0\n\nMatthews Correlation Coefficient (MCC)\u00a0\n\n\u00a0 The MCC is a global measure that takes into account all the four elements of the confusion matrix and is especially informative in the case of binary authenticity detection.\u00a0\n\n\t\n\n\t(7)\n\n\u00a0 The scale of MCC values is -1 to +1 with +1 being perfect prediction, 0 random performance, and -1 complete disagreement.\u00a0\n\nArea Under the Curve (AUC)\u00a0\n\n\u00a0 The AUC, computed from the Receiver Operating Characteristic (ROC) curve, measures the model's discriminative power by evaluating its ability to separate positive and negative classes across all possible thresholds.\u00a0\n\n\t\n\n\t(8)\n\n\u00a0 Greater AUC (near to 1.0) implies a great separability and certainty in the authenticity predicted probabilities.\u00a0\n\nBootstrapped Confidence Intervals\u00a0\n\n\u00a0 To guarantee the statistical strength, the results of all significant performance measures: accuracy, F1-Score, and AUC, were averaged with 5 bootstrap resampling runs and 95% confidence intervals were provided where possible. This method alleviates the issue of sample bias and confirms the stability of generalization of the suggested framework.\u00a0\n\nEmotion Recognition Results\u00a0\n\n\u00a0 The initial subtask of the proposed dual-task framework is facial emotion recognition. This test is based on the capability of the model to categorize seven discrete emotion types accurately in a controlled testing condition. The evaluation of performance was based on the use of conventional classification metrics, confusion of analysis and convergence attributes over the training epochs. All classes achieved higher than 0.95 in terms of the F1-score, and Fear, Happy, Surprise obtained a perfect F1-score (i.e., 1.00).\u00a0\n\n\u00a0 In addition, the Cohen\u2019s Kappa (0.982) and MCC (0.982) show high inter class reliability, which confirms that the model is fairly robust to deal with class imbalance.\u00a0\n\nThis high result is explained by the discriminative ability of EfficientNet-B0 backbone that successfully captures hierarchical facial features, as well as SE attention that prioritizes emotionally salient areas.", "mimetype": "text/plain", "start_char_idx": 35844, "end_char_idx": 40464, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "db074777-b2b5-4192-87c6-7f3ee7c1b029": {"__data__": {"id_": "db074777-b2b5-4192-87c6-7f3ee7c1b029", "embedding": null, "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37", "node_type": "4", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "ff9b494f06ab2d9301cf95741443fa088208aac93ccbbf897ba21c90e3f8ad47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "54909c00-6b3d-4496-b69d-72110ea564e5", "node_type": "1", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "e016ccb0eb0e2eabc84efeb355e5ea63ed875bb0489c7541201ce426944480d5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "888bede3-ab79-4dfe-8da7-19b862e6672a", "node_type": "1", "metadata": {}, "hash": "3bdc3a06feb7119f5ff5abd48c8dc16193c297c419842cb38dac4262622bfb95", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Emotion Recognition Results\u00a0\n\n\u00a0 The initial subtask of the proposed dual-task framework is facial emotion recognition. This test is based on the capability of the model to categorize seven discrete emotion types accurately in a controlled testing condition. The evaluation of performance was based on the use of conventional classification metrics, confusion of analysis and convergence attributes over the training epochs. All classes achieved higher than 0.95 in terms of the F1-score, and Fear, Happy, Surprise obtained a perfect F1-score (i.e., 1.00).\u00a0\n\n\u00a0 In addition, the Cohen\u2019s Kappa (0.982) and MCC (0.982) show high inter class reliability, which confirms that the model is fairly robust to deal with class imbalance.\u00a0\n\nThis high result is explained by the discriminative ability of EfficientNet-B0 backbone that successfully captures hierarchical facial features, as well as SE attention that prioritizes emotionally salient areas. The regulated data collection procedure also minimized the noise associated with pose and illumination allowing better differentiation of emotion categories. All these factors contribute to the high precision and recall rates that have been constantly achieved in all classes.\n\nSummary of Classification Performance\u00a0\u00a0\n\n\u00a0 The model performed well for the seven primary emotion classes. From Table 5, we can see that the final accuracy performance is 98.5%, the macro F1-score is 0.983 and the macro F1-score is 0.985.\u00a0\u00a0\n\nTable 5. Classification results for Emotion Recognition in the seven categories.\n\nEmotion\u00a0\n\nPrecision\u00a0\n\nRecall\u00a0\n\nF1-Score\u00a0\n\nSupport\u00a0\n\nAngry\u00a0\n\n1.00\u00a0\n\n0.92\u00a0\n\n0.96\u00a0\n\n36\u00a0\n\nDisgust\u00a0\n\n0.93\u00a0\n\n1.00\u00a0\n\n0.96\u00a0\n\n40\u00a0\n\nFear\u00a0\n\n1.00\u00a0\n\n1.00\u00a0\n\n1.00\u00a0\n\n59\u00a0\n\nHappy\u00a0\n\n1.00\u00a0\n\n1.00\u00a0\n\n1.00\u00a0\n\n60\u00a0\n\nSad\u00a0\n\n0.95\u00a0\n\n1.00\u00a0\n\n0.98\u00a0\n\n40\u00a0\n\nSurprise\u00a0\n\n1.00\u00a0\n\n1.00\u00a0\n\n1.00\u00a0\n\n27\u00a0\n\nNeutral\u00a0\n\n1.00\u00a0\n\n0.97\u00a0\n\n0.99\u00a0\n\n70\u00a0\n\nOverall\u00a0\n\n\u00a0\n\n\u00a0\n\n0.985\u00a0\n\n332\u00a0\n\nOverall F1-Score is calculated by taking the weighted average across all classes.\u00a0\n\nConfusion Matrix Analysis\u00a0\n\nThe confusion matrix, in Fig. 11, of the seven categories of emotions shows the errors in the classification distribution. The majority of categories obtained close-to-perfect discrimination; however, most of the misclassifications happened in Angry (92% recall) and Neutral (97% recall), as they have slight changes from normal facial expression which are confusable across different categories. This pattern indicates known overlaps in facial muscle activation between low-arousal emotions, indicating that remaining errors are largely perceptual rather than structural model failures.\u00a0\u00a0\n\nFigure 11. Confusion matrix of seven class emotion recognition task of the proposed model.\n\nTraining and Validation Curves Analysis\u00a0\n\nIn order to evaluate the learning stability of the emotion recognition branch, the training and validation accuracy curves are represented in Fig. 12. Both curves show a smooth convergence, with the validation accuracy saturating at about 98% and no significant gap between training and validation, indicating minimal overfitting. This consistent trend supports the high ability of the SE-augmented EfficientNet-B0 backbone to generalize, even with moderate data requirements.\u00a0\n\nFigure 12. Training and validation accuracy curves for emotion recognition\n\nAuthenticity Detection Results\u00a0\n\n\u00a0 Authenticity detection, which distinguishes genuine from acted expressions, constitutes the second and more challenging subtask. In contrast to categorical FER, this problem demands a sensitivity to finer, more subtle cues in facial musculature and temporal dynamics. Thus, we assessed the task in terms of not only the traditional accuracy and F1-scores, but also of reliability measures, namely, Cohens Kappa, MCC, and bootstrapped confidence intervals.\u00a0\n\nSummary of Classification Performance\u00a0\u00a0\n\n\u00a0 Authenticity detection was more challenging because of fine-grained differences between authentic and faked expressions. The model obtained an accuracy of 92.2% and a macro F1-score of 0.92 (Table 6).", "mimetype": "text/plain", "start_char_idx": 39523, "end_char_idx": 43519, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "888bede3-ab79-4dfe-8da7-19b862e6672a": {"__data__": {"id_": "888bede3-ab79-4dfe-8da7-19b862e6672a", "embedding": null, "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37", "node_type": "4", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "ff9b494f06ab2d9301cf95741443fa088208aac93ccbbf897ba21c90e3f8ad47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "db074777-b2b5-4192-87c6-7f3ee7c1b029", "node_type": "1", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "261c58f325529d2ea936a4a5681aeac51afaadcebdad2af33f018962374c92f3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a0925b83-532c-4cae-897d-04622286f332", "node_type": "1", "metadata": {}, "hash": "08d5c7272b914b682f9b1dc23fa7b97d04170f8a9003f53ba5472698aa702a2b", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "This consistent trend supports the high ability of the SE-augmented EfficientNet-B0 backbone to generalize, even with moderate data requirements.\u00a0\n\nFigure 12. Training and validation accuracy curves for emotion recognition\n\nAuthenticity Detection Results\u00a0\n\n\u00a0 Authenticity detection, which distinguishes genuine from acted expressions, constitutes the second and more challenging subtask. In contrast to categorical FER, this problem demands a sensitivity to finer, more subtle cues in facial musculature and temporal dynamics. Thus, we assessed the task in terms of not only the traditional accuracy and F1-scores, but also of reliability measures, namely, Cohens Kappa, MCC, and bootstrapped confidence intervals.\u00a0\n\nSummary of Classification Performance\u00a0\u00a0\n\n\u00a0 Authenticity detection was more challenging because of fine-grained differences between authentic and faked expressions. The model obtained an accuracy of 92.2% and a macro F1-score of 0.92 (Table 6). Performance on fake expressions was marginally better (recall = 0.942, F1 = 0.926) than on genuine ones (recall = 0.899, F1 = 0.917), implicate that acted expressions have exaggerated visual cues that the model can more easily capture.\u00a0\n\n\u00a0 Robustness was also demonstrated as shown by a Cohen\u2019s Kappa of 0.843, and MCC factor of 0.843 (with substantial agreement). The bootstrapped assessment achieved an average accuracy of 0.922 with a 95% CI of 0.892\u20130.949, establishing the statistical validity of the performance being reported.\n\nAuthenticity detection is based on finer spatial indicators, as opposed to discrete facial configurations, when compared to emotion recognition. This explains the comparatively lower accuracy, while still demonstrating strong reliability metrics, confirming that the shared emotion-aware representation provides meaningful contextual support for authenticity inference.\u00a0\n\nTable 6. Classification performance for authenticity detection (genuine vs. fake).\n\nClass\u00a0\n\nPrecision\u00a0\n\nRecall\u00a0\n\nF1-Score\u00a0\n\nSupport\u00a0\n\nFake\u00a0\n\n0.91\u00a0\n\n0.94\u00a0\n\n0.93\u00a0\n\n173\u00a0\n\nGenuine\u00a0\n\n0.93\u00a0\n\n0.90\u00a0\n\n0.92\u00a0\n\n159\u00a0\n\nOverall\u00a0\n\n\u00a0\n\n\u00a0\n\n0.922\u00a0\n\n332\u00a0\n\n\u00a0 Metrics are presented as average values over five bootstrap runs with 95% CI(0.892-0.949).\u00a0\n\nConfusion Matrix Analysis\u00a0\n\nThe confusion matrix of authenticity detection, as shown in Fig. 13, indicates that the model has a high level of discrimination between genuine and fake expressions. Of the total number of fake samples, 163 were rightfully identified, with only 10 falsely identified as genuine. On the same note, 143 actual samples were recognized correctly, and 16 were wrongly classified as fake. It implies that the model is highly accurate and recalls both classes and can capture authenticity cues. The minor instances of misclassification probably are attributed to the nuanced or ambiguous facial expression that contain similar visual features of the real and fake emotions. These misclassifications are likely caused by borderline expressions where acted emotions closely mimic spontaneous facial dynamics, highlighting the intrinsic difficulty of authenticity assessment from static imagery.\u00a0\u00a0\n\n\u00a0Figure 13. Confusion matrix of authenticity detection (genuine vs. fake).\n\nROC Curve Analysis\u00a0\n\nThe Receiver Operating Characteristic (ROC) curve as shown in Fig. 14 analyzes the relationship between the true-positive and false-positive rates at various thresholds. The authenticity branch scored a 0.922 AUC, indicating that there is high discriminative power between genuine and fake expressions. A high sensitivity and specificity ratio are indicated by the smooth and steeply rising ROC profile toward the upper-left corner, which confirms the credibility of the model confidence estimates.\u00a0\n\nFigure 14. ROC curves for authenticity detection.\n\nPrecision\u2013Recall Curve Analysis\u00a0\n\nThe discriminative capability of the authenticity detection branch is further confirmed by the Precision-Recall (PR) curve depicted in Fig. 15. The model had an Average Precision (AP) of 0.97, which shows that precision is almost ideal in a broad recall threshold. The curve maintains a flat, high-precision plateau before a gradual decline at extreme recall values, signifying balanced sensitivity and reliability even under imbalanced class distributions. These results are consistent with the results of the ROC-based framework and validate the strength of the dual-task framework in authenticity classification and is especially valuable to real-world application, where inaccurate authenticity prediction can be both ethically and practically harmful.\u00a0\u00a0\n\nFigure 15.", "mimetype": "text/plain", "start_char_idx": 42559, "end_char_idx": 47128, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a0925b83-532c-4cae-897d-04622286f332": {"__data__": {"id_": "a0925b83-532c-4cae-897d-04622286f332", "embedding": null, "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37", "node_type": "4", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "ff9b494f06ab2d9301cf95741443fa088208aac93ccbbf897ba21c90e3f8ad47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "888bede3-ab79-4dfe-8da7-19b862e6672a", "node_type": "1", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "b7a7e293cc1cdd42e99650bf6dbae7dc1486aee45bccdfe15696eb23298c85ea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ad3554ed-c72c-4d81-ab91-1c7e3ce95f82", "node_type": "1", "metadata": {}, "hash": "a8caa25008d6d1c3833f22bb83f35c7d6e3d87f132e7ebca866ac6be13c58e32", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A high sensitivity and specificity ratio are indicated by the smooth and steeply rising ROC profile toward the upper-left corner, which confirms the credibility of the model confidence estimates.\u00a0\n\nFigure 14. ROC curves for authenticity detection.\n\nPrecision\u2013Recall Curve Analysis\u00a0\n\nThe discriminative capability of the authenticity detection branch is further confirmed by the Precision-Recall (PR) curve depicted in Fig. 15. The model had an Average Precision (AP) of 0.97, which shows that precision is almost ideal in a broad recall threshold. The curve maintains a flat, high-precision plateau before a gradual decline at extreme recall values, signifying balanced sensitivity and reliability even under imbalanced class distributions. These results are consistent with the results of the ROC-based framework and validate the strength of the dual-task framework in authenticity classification and is especially valuable to real-world application, where inaccurate authenticity prediction can be both ethically and practically harmful.\u00a0\u00a0\n\nFigure 15. Precision-Recall curve of authenticity detection.\n\nTraining and Validation Curves Analysis\u00a0\u00a0\n\nFig 16, shows the training and validation curves of the authenticity task, which converge steadily with little variance between the training and validation sets. This behavior indicates the proposed framework has been able to reduce overfitting, although authenticity detection is complex by nature.\u00a0\u00a0\n\nFigure 16. Training and validation accuracy/loss curves for authenticity detection.\n\nOverall Training Dynamics\u00a0\n\n\u00a0 In order to examine the joint learning behavior of the dual-task model, the combined training and validation accuracy and loss curves are illustrated in Fig. 17. The accuracy pattern shows overall improvement in both branches, though with a higher rate of convergence of the emotion task because of the stronger categorical supervision. In the meantime, the authenticity task displays slower yet consistent increases, which can be attributed to the more subtle inter-class differences in the real versus fake expressions.\u00a0\n\n\u00a0 The associated loss curves show a non-varying monotonically decreasing curve without oscillations, which is indicative of stabilized optimization and lack of divergence. The minimal difference between training and validation losses shows the efficient regularization and strong generalization of the dropout, SE-attention, and stochastic-depth modules. This stability demonstrates the effectiveness of joint loss optimization and SE-based feature recalibration in balancing task dominance during training.\u00a0\u00a0\n\nFigure 17. Combined training and validation accuracy and loss curves for both emotion recognition and authenticity detection task.\n\nComparative Evaluation\u00a0\n\n\u00a0 To validate the effectiveness of the proposed dual-task framework, we compared it against several baseline and hybrid configurations: CNN, EfficientNetB0 + MobileNetV2, MobileNetV2 + Xception, and EfficientNet-B3. The summarized results in Table 7 and confusion matrices in Fig. 18 (i\u2013v) demonstrate the superiority of our proposed model, which achieved 98.5% emotion accuracy and 92.2% authenticity AUC. These results highlight the advantage of attention-enhanced dual-branch optimization to classical architectures.\n\nThe dual-task EfficientNet-B0 with attention achieved consistent improvement over all the baselines, especially in authenticity detection task, where the majority of the single-branch models were challenged by class imbalance and less discriminatory power. However, while MobileNetV2+Xception and EfficientNet-B3 embeddings offered modest gains over a vanilla CNN baseline, the generalization they achieved was limited in comparison with the dual-task approach proposed in this study.\u00a0\n\nThe CNN baseline (Fig. 18-i) shows many misclassifications between Angry\u2013Sad and Neutral\u2013Disgust. Performance of authenticity prediction is roughly balanced for fake and genuine samples but separation is limited. The EfficientNet-B0 + MobileNetV2 combinatorial architecture (Fig. 18-ii) provides an emotion recognition accuracy gain while still suffering from relatively high confusion especially between visually similar emotions, such as Neutral and Sad. In the authenticity task, all samples were biased towards genuine class, which resulted in high recall and very low discrimination (that is weak inter-branch feature alignment and poor depiction of authenticity).\n\nThe EfficientNet-B3 embedding model (Fig. 18-iii) performs fair with a moderate bias and discrimination among emotions. However, authenticity scores show low levels of sensitivity as frozen backbone embeddings have insufficient representations at the fine-grained level. The MobileNetV2 + Xception fusion (Fig. 18-iv) increases the diversity of feature embeddings that denoising improves the recall rate for Fear and Happy but it cannot be generalized well to Surprise and Sad. There still exists moderate authenticity confusion, some real samples are misclassified as fake (which can be attributed to the redundancy in cross-model fusion).", "mimetype": "text/plain", "start_char_idx": 46075, "end_char_idx": 51152, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "ad3554ed-c72c-4d81-ab91-1c7e3ce95f82": {"__data__": {"id_": "ad3554ed-c72c-4d81-ab91-1c7e3ce95f82", "embedding": null, "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37", "node_type": "4", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "ff9b494f06ab2d9301cf95741443fa088208aac93ccbbf897ba21c90e3f8ad47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a0925b83-532c-4cae-897d-04622286f332", "node_type": "1", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "e1c8e30b39b7b64a79c1ced9f8be8960ee1a9736e44216a141f95141d517ab93", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a45537ac-7c75-4531-93d4-50614f3ad27b", "node_type": "1", "metadata": {}, "hash": "39034ba8ed7b12d34c92fab3e2aa1561fbe69c6491e59c216991d5d81eb247c2", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "18-ii) provides an emotion recognition accuracy gain while still suffering from relatively high confusion especially between visually similar emotions, such as Neutral and Sad. In the authenticity task, all samples were biased towards genuine class, which resulted in high recall and very low discrimination (that is weak inter-branch feature alignment and poor depiction of authenticity).\n\nThe EfficientNet-B3 embedding model (Fig. 18-iii) performs fair with a moderate bias and discrimination among emotions. However, authenticity scores show low levels of sensitivity as frozen backbone embeddings have insufficient representations at the fine-grained level. The MobileNetV2 + Xception fusion (Fig. 18-iv) increases the diversity of feature embeddings that denoising improves the recall rate for Fear and Happy but it cannot be generalized well to Surprise and Sad. There still exists moderate authenticity confusion, some real samples are misclassified as fake (which can be attributed to the redundancy in cross-model fusion).\n\nOn the other hand, dual-branch EfficientNet-B0 architecture without SE (Fig. 18-v) exhibits significant confusion in the authenticity detection which verifies that lack of SE attention undermines spatial recalibration effectiveness.\n\nConversely, the presented dual-branch model was able to minimize these confusions by using squeeze-and-excitation (SE) attention and combined emotion and authenticity branch optimization. This improvement allowed a higher inter-class separability and enhanced generalization when there was imbalance between the classes.\n\nOverall, these comparisons confirm that neither deeper architectures nor feature fusion alone are sufficient for authenticity modeling. Instead, the combination of dual-task supervision and attention-guided feature refinement is critical for learning discriminative authenticity cues. This explains the consistent superiority of the proposed model across both emotion recognition and authenticity detection tasks.\u00a0\u00a0\n\nModel\n\nEmotion Accuracy (%)\n\nEmotion F1 (%)\n\nAuthenticity AUC (%)\n\nAuthenticity F1 (%)\n\nCNN (baseline)\n\n59.0\n\n59.0\n\n69.0\n\n69.0\n\nEfficientNetB0 + MobileNetV2\n\n82.0\n\n82.0\n\n53.0\n\n37.0\n\nEfficientNet-B3 (embed + dense)\n\n62.0\n\n62.0\n\n64.0\n\n63.0\n\nMobileNetV2 + Xception (embedding)\n\n64.0\n\n63.0\n\n67.0\n\n67.0\n\nEfficientNet-B0 (dual-branch)\n\n82.0\n\n82.0\n\n52.0\n\n50.0\n\nProposed Dual-Task (EfficientNet-B0 + SE Attention)\n\n98.5\n\n98.3\n\n92.2\n\n92.2\n\nTable 7. Comparison of performance between baseline models and proposed dual-task model\n\nBaseline results are achieved by direct application of classification reports, and the proposed model integrates attention-enhanced branches with joint loss optimization.\u00a0\u00a0\n\nCNN (Baseline).\n\nEfficientNet-B0 + MobileNetV2 (Hybrid Branch).\n\nEfficientNet-B3 (Feature Embedding + Dense Classifier).\n\nMobileNetV2 + Xception (Embedding Fusion.)\n\nEfficientNet-B0 (Dual-Branch Without SE).\n\nFigure 18. Comparative confusion matrices of emotion and authenticity recognition across models.\n\nComparison with Existing Studies\u00a0\n\nThough there are a number of recent studies that have examined facial emotion recognition and genuine-fake emotion detection separately, there is no direct one-to-one comparison with previous literature because there are no existing frameworks upto our knowledge that can jointly tackle both tasks in a single architecture. Because of this, and in accordance with the previous assessment tradition, the given solution is contrasted individually with the corresponding recent works in the field of emotion recognition and authenticity detection, which are summarized in Table 8.\u00a0\u00a0\n\nTable 8. Comparison of the proposed approach with existing facial emotion recognition and authenticity detection studies\n\nStudy\n\nTask\n\nDataset\n\nModel / Approach\n\nClasses\n\nReported Performance\n\nLimitations\n\n[11] Anand & Babu\n\nEmotion Recognition\n\nFER2013,\n\nEMOTIC\n\nEfficientNet-B0\n\n(optimized)\n\n7\n\n96%\n\nSingle-task, no authenticity\n\n[12] Manimohan et al.\n\nEmotion Recognition\n\nFER2013,\n\nCustom\n\nCNNS, ResNet50\n\n7, 5\n\n79.38%\n\nSensitive to pose & lighting\n\n[13] Khuntia & Kale\n\nEmotion Recognition\n\nFER2013, FER2013+\n\nCNN, LSTM, ResNet18, and QDA + PCA\n\n7\n\n79.72%\n\nNo interpretability\n\n[23] Sunil et al.\n\nAuthenticity Detection\n\nChaLearn, Fake smile master, and Custom\n\nResNet\n\nBinary\n\n96%\n\nNo emotion modeling\n\n[24] Arslan et al.", "mimetype": "text/plain", "start_char_idx": 50121, "end_char_idx": 54471, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "a45537ac-7c75-4531-93d4-50614f3ad27b": {"__data__": {"id_": "a45537ac-7c75-4531-93d4-50614f3ad27b", "embedding": null, "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37", "node_type": "4", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "ff9b494f06ab2d9301cf95741443fa088208aac93ccbbf897ba21c90e3f8ad47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ad3554ed-c72c-4d81-ab91-1c7e3ce95f82", "node_type": "1", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "c0ee9847f3b1922a07468bcbcb45d682c15277e7b1d1995d7220d5d33941266c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "adf94ef1-93ab-40e8-9487-9bbe9f52a8ce", "node_type": "1", "metadata": {}, "hash": "63ef9d86d164153876437622a754d0c103eeb62905d5372c0d86d2a7cebbd627", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "Emotion Recognition\n\nFER2013,\n\nCustom\n\nCNNS, ResNet50\n\n7, 5\n\n79.38%\n\nSensitive to pose & lighting\n\n[13] Khuntia & Kale\n\nEmotion Recognition\n\nFER2013, FER2013+\n\nCNN, LSTM, ResNet18, and QDA + PCA\n\n7\n\n79.72%\n\nNo interpretability\n\n[23] Sunil et al.\n\nAuthenticity Detection\n\nChaLearn, Fake smile master, and Custom\n\nResNet\n\nBinary\n\n96%\n\nNo emotion modeling\n\n[24] Arslan et al.\n\nAuthenticity Detection\n\nVREED\n\nMultimodal (ECG + GSR)\n\n4\n\n97.78%\n\nRequires advance hardwares\n\nProposed Method\n\nEmotion and Authenticity\n\nCustom (psychologist validated)\n\nEfficientNet-B0 + SE (Dual-Task)\n\n7 and Binary\n\n98.5% and 92.2%\n\nStatic images only\n\n\n\n\u00a0 In the domain of facial emotion recognition, contemporary deep learning-based approaches typically report accuracies ranging from approximately 85% to 96%, depending on dataset characteristics, number of emotion categories, and model complexity. Methods employing CNN backbones such as ResNet, MobileNet, and EfficientNet have shown strong performance under both controlled and in-the-wild conditions [11], [12], [13]. Compared to these works, the proposed model achieves a higher accuracy of 98.5% and a macro-F1 score of 0.983 across seven emotion classes. This performance gain can be attributed to the incorporation of squeeze-and-excitation attention, which enhances channel-wise feature discrimination, as well as the auxiliary supervision introduced by the authenticity detection task, encouraging the shared representation to focus on psychologically meaningful facial regions.\u00a0\n\n\u00a0 For authenticity detection, prior studies have largely framed the problem as a standalone binary classification task, often relying on handcrafted features or single-task CNN architectures. Reported accuracies in recent image-based approaches typically fall within the range of 88%\u201397% [23], [24]. While some video-based or micro-expression\u2013driven methods have achieved incremental improvements, these approaches generally depend on temporal information or specialized datasets that limit practical deployment [18], [19]. In contrast, the proposed framework attains 92.2% accuracy and a macro-F1 score of 0.92 using only static facial images, suggesting that joint learning of emotion categories and authenticity cues enables the model to capture subtle inconsistencies in facial expressions that are less accessible in single-task settings.\n\nIt is important to emphasize that variations in datasets, annotation protocols, and evaluation strategies restrict strict numerical comparisons across studies. Most of the existing studies use publicly available datasets having posed expressions or small demographic diversity, but the current study uses a psychologist-validated dataset that was selected to be used in authenticity analysis. In spite of these disparities, the overall improvements that have been reported as compared to the latest standards show that the suggested dual-task model is a significant development. As far as the authors are aware, the present research is one of the first to combine the multi-class emotion recognition and authenticity detection into one attention-enhanced deep learning framework.\n\nDeployment Validation\u00a0\n\n\u00a0 The trained dual-task model, which is an extension of EfficientNet-B0 using squeeze-and-excitation (SE) attention, was applied online in a real-time inference pipeline on Render. It was trained using live video input streams and combined face detection, alignment, dual-task inference and result visualization. The pipeline has three major steps:\u00a0\n\n\u00a0(i) detection of a face with the help of the MediaPipe framework provided by Google, (ii) the dual-task inference to recognize categorical emotions and predict authenticity, and (iii) visualization of the results with the help of a web-based interface\u00a0\u00a0\n\n\u00a0 The deployment in the cloud was based on a CPU-only environment but was able to support an average processing rate of about 5 frames per second, and this was adequate to support smooth interaction over real-time. An image of the\u00a0\u00a0\n\nlive interface is provided in Fig. 19, in which a set of bounding boxes, predicted emotion-based categories, and authenticity labels are overlaid on top of the video feed. This empirical confirmation reveals that the given model can be applied to e-learning analytics, security surveillance, and human-computer interaction.\u00a0\u00a0\n\nFigure 19. Render hosted real time deployment interface.\n\nExplainable AI for Model Interpretation\n\n  Although deep learning models have a high predictive power, they are frequently considered black-box systems that cannot be easily interpreted. With applications that require emotion and authenticity detection, it is important that there is trust and transparency, especially in sensitive fields like security, healthcare, and human-AI interaction.", "mimetype": "text/plain", "start_char_idx": 54099, "end_char_idx": 58887, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "adf94ef1-93ab-40e8-9487-9bbe9f52a8ce": {"__data__": {"id_": "adf94ef1-93ab-40e8-9487-9bbe9f52a8ce", "embedding": null, "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37", "node_type": "4", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "ff9b494f06ab2d9301cf95741443fa088208aac93ccbbf897ba21c90e3f8ad47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a45537ac-7c75-4531-93d4-50614f3ad27b", "node_type": "1", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "5ab11c66cebaf79551ecdaade8810c782eecec5dde05978a1d3ada8f8270caad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2f708ad4-353c-413c-b574-74f0eacb9fc6", "node_type": "1", "metadata": {}, "hash": "34a6905774a7eda6fb502fbe90fc3ee9d355bbee1242d7aab0ecb3d256e737c1", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "An image of the\u00a0\u00a0\n\nlive interface is provided in Fig. 19, in which a set of bounding boxes, predicted emotion-based categories, and authenticity labels are overlaid on top of the video feed. This empirical confirmation reveals that the given model can be applied to e-learning analytics, security surveillance, and human-computer interaction.\u00a0\u00a0\n\nFigure 19. Render hosted real time deployment interface.\n\nExplainable AI for Model Interpretation\n\n  Although deep learning models have a high predictive power, they are frequently considered black-box systems that cannot be easily interpreted. With applications that require emotion and authenticity detection, it is important that there is trust and transparency, especially in sensitive fields like security, healthcare, and human-AI interaction. We used two explainable AI (XAI) methods popular with other scholars (Gradient-weighted Class Activation Mapping (Grad-CAM) and Local Interpretable Model-agnostic Explanations (LIME)) to offer post-hoc interpretability of the proposed dual-task model. The two approaches were implemented following model assessment, which ensures that the explanations are based on the real-life decision-making behavior of the trained network.\n\nGrad-CAM for Visual Explanations\n\n  Grad-CAM produces class-discriminative localization maps which indicate the most salient parts of the input that are used to make a particular decision. When used with emotion recognition, Grad-CAM repeatedly prioritized physiologically important locations, including the eyes, mouth, and eyebrows. In the case of authenticity detection, Grad-CAM attention was directed to micro-expressions and peripheral features, which meant that the model was trained to observe small dynamics of the face that differentiate between real and fake expressions.\n\n The resulting heatmaps illustrating these activation patterns are presented in Fig. 20, which visualizes the key regions influencing model predictions for both real and fake emotion samples.\n\nFigure 20. Grad-CAM visualizations showing discriminative facial regions for real and fake expressions.\n\nLIME for Local Feature Attribution\n\nWhereas Grad-CAM offers global spatial explanation, LIME supplements this by providing feature importance on local prediction. When applied to detecting authenticity, LIME emphasized fine-textural differences, wrinkles and shading patterns, which affected classification. This view offered by LIME complements Grad-CAM to justify local evidence in order to make decisions of authenticity.\n\nAn example of these LIME-based local feature attributions is shown in Fig. 21, which highlights pixel-level evidence differentiating genuine and fake facial expressions across emotion categories.\n\nFigure 21.  LIME plots of local feature significance between genuine and fake emotion samples\n\nComplementary Insights and Reliability\n\nGrad-CAM and LIME jointly give explanations that are complementary to each other: Grad-CAM focuses on where the model is paying attention in the face, and LIME shows what local features are causing the model to make predictions. These model explanations are consistent with human-decodable cues (e.g., eyebrows pulling together due to anger, smiling lips on a fake smile) and are consistent with established psychological theories of emotional authenticity. Such a layer of interpretability fosters the trust of the offered system and contributes to its ethical utilization within the real-life context.\n\nConclusion\n\n  This paper introduced a dual-task model that integrates the recognition of facial emotions and authenticity detection based on an EfficientNet-B0 backbone with SE attention. The combination of categorical emotion modeling and validity cues modeling by the system provided a close solution to an important limitation in the field of affective computing that exists in the standard single-task FER methods. Built upon an EfficientNet-B0 backbone enhanced with squeeze-and-excitation attention, the proposed model was designed to learn both categorical emotional patterns and subtle authenticity-related cues within a shared representation. A carefully curated dataset, annotated under the supervision of a licensed psychologist, enabled systematic learning of genuine and fake expressions across seven universal emotion categories. \n\n  The experiments showed good results: emotion recognition was 98.5% accurate with a macro-F1 score of 0.983, with several emotion classes attaining near-perfect performance and very high inter-class reliability (Cohen\u2019s Kappa = 0.982). Authenticity detection, which was more inherently more challenging due to subtle expressive differences, had the highest accuracy of 92.2% and a macro-F1 score of 0.92, with substantial agreement (Kappa = 0.843). These findings verify the strength of the shared representation and at the same time, the challenge of separating the true and fake expressions. \n\nBased on comparative analysis, there were steady improvements across baselines, with the most significant improvement on authenticity classification, which highlights the advantage of learning multiple tasks and integrating attention.", "mimetype": "text/plain", "start_char_idx": 58092, "end_char_idx": 63239, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "2f708ad4-353c-413c-b574-74f0eacb9fc6": {"__data__": {"id_": "2f708ad4-353c-413c-b574-74f0eacb9fc6", "embedding": null, "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37", "node_type": "4", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "ff9b494f06ab2d9301cf95741443fa088208aac93ccbbf897ba21c90e3f8ad47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "adf94ef1-93ab-40e8-9487-9bbe9f52a8ce", "node_type": "1", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "5606c66bf02f109a4f70cec15ab023f6c0ec145aa5a635261f6d74c1930ddafd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d71f7c2-63ad-46c0-a68b-86a68fca7f25", "node_type": "1", "metadata": {}, "hash": "a7a4c7f6ec4cbabda152d323a8ee271815a1407365ba0ed43fb45aa08dcf322e", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "A carefully curated dataset, annotated under the supervision of a licensed psychologist, enabled systematic learning of genuine and fake expressions across seven universal emotion categories. \n\n  The experiments showed good results: emotion recognition was 98.5% accurate with a macro-F1 score of 0.983, with several emotion classes attaining near-perfect performance and very high inter-class reliability (Cohen\u2019s Kappa = 0.982). Authenticity detection, which was more inherently more challenging due to subtle expressive differences, had the highest accuracy of 92.2% and a macro-F1 score of 0.92, with substantial agreement (Kappa = 0.843). These findings verify the strength of the shared representation and at the same time, the challenge of separating the true and fake expressions. \n\nBased on comparative analysis, there were steady improvements across baselines, with the most significant improvement on authenticity classification, which highlights the advantage of learning multiple tasks and integrating attention. In addition to accuracy, explainable AI techniques (Grad-CAM and LIME) showed that the model emphasized semantically relevant areas like the mouth and eyes, which is transparent and is consistent with the evidence of psychology. \n\nDespite these promising results, certain limitations remain. There are constraints such as limited diversity of datasets, lack of time or multimodal information, which could limit the authenticity modeling. Though the dataset was validated by a licensed psychologist, the number of participants (46 subject) is limited, resulting incomplete capture of cross-cultural, age-related, and spontaneous expression variations. Therefore the model is expected to generalize best within controlled or semi-controlled environments. Future directions include larger, culturally diverse, and in-the-wild datasets are expected to further improve robustness and external validity. Viable use was also validated through deployment. A pipeline based on FastAPI with MediaPipe detection reached approximately 5 FPS on cloud CPU servers, which can be used to support real-time use in e-learning, surveillance, and human-AI interaction. In short, the suggested dual-task model enhances affective computing by integrating emotion and authenticity with both high precision, interpretability, and deployability, which will be the basis of more transparent and trustful human-AI systems.\n\nDeclarations\n\nAuthor Contributions\n\nThe following statements should be used: \n\nConceptualization, S.T.D., M.J.F, M.M.R; methodology, S.T.D., M.J.F, M.M.R; software, S.T.D., M.J.F; validation, M.M.R., S.A and O.; formal analysis, M.M.R, S.A; investigation, O.; resources, S.T.D. and M.JF.; data curation, S.T.D., M.M.J., M.M.R.; writing\u2014original draft preparation, S.T.D., M.J.F; writing\u2014review and editing, M.M.R., S.A., O.; visualization, M.M.R., O.; supervision, M.M.R., O; project administration, S.T.D., M.M.R.; funding acquisition, S. A., O.;All authors have read and agreed to the published version of the manuscript.\n\nData Availability Statement\n\nData available in a publicly accessible repository: The data presented in this study are openly available in Mendeley Data: Diya, Sarah Tasnim; Ferdos, Most Jannatul; Rahman, Md Mizanur (2025), \u201cGenuine and Fake Facial Emotion Dataset (GFFD-2025)\u201d, Mendeley Data, V1, doi: 10.17632/wmfd4p3z32.1, reference number [34].\n\nFunding\n\nFunding: This study was funded by the Institute for Advanced Research Publication Grant of United Interna-\n\ntional University, Ref. No.: IAR-2025-Pub-000.\n\nAcknowledgements\n\n We would like to express our sincere gratitude to the Department of Computer Science & Engineering and the Faculty of Science & Information Technology (FSIT), Daffodil International University (DIU), for providing continuous research support and a conducive research environment. We are also thankful to the Department of CSE, United International University (UIU), for their valuable academic support. Additionally, we gratefully acknowledge the psychologist team of Daffodil International University for their assistance in data verification during this research.\n\nInstitutional Review Board Statement\n\nThis research was ethically approved by the Department of Computer Science & Engineering, Daffodil International University (DIU). The study was also formally granted approval as a final-year defense thesis by the Faculty of Science & Information Technology (FSIT), DIU.", "mimetype": "text/plain", "start_char_idx": 62214, "end_char_idx": 66670, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "1d71f7c2-63ad-46c0-a68b-86a68fca7f25": {"__data__": {"id_": "1d71f7c2-63ad-46c0-a68b-86a68fca7f25", "embedding": null, "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37", "node_type": "4", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "ff9b494f06ab2d9301cf95741443fa088208aac93ccbbf897ba21c90e3f8ad47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f708ad4-353c-413c-b574-74f0eacb9fc6", "node_type": "1", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "5cd17f4051250373bff077b181c5e5d8d37907612cba4632395d398d654d9f0e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7ad8d4be-c2e0-44d7-aed5-575ec7d380f8", "node_type": "1", "metadata": {}, "hash": "16800f72f3ecb81e956f514b095b5404f9cef7d50209f1e31182642848a5dc8c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "No.: IAR-2025-Pub-000.\n\nAcknowledgements\n\n We would like to express our sincere gratitude to the Department of Computer Science & Engineering and the Faculty of Science & Information Technology (FSIT), Daffodil International University (DIU), for providing continuous research support and a conducive research environment. We are also thankful to the Department of CSE, United International University (UIU), for their valuable academic support. Additionally, we gratefully acknowledge the psychologist team of Daffodil International University for their assistance in data verification during this research.\n\nInstitutional Review Board Statement\n\nThis research was ethically approved by the Department of Computer Science & Engineering, Daffodil International University (DIU). The study was also formally granted approval as a final-year defense thesis by the Faculty of Science & Information Technology (FSIT), DIU. The research project received official ethical and technical approval and was conducted under the institutional reference ID PMS-CSE/DIU-SP25D65423.\n\nInformed Consent Statement\n\nThis study involves the use of facial expression data, which required capturing human facial images. All participants officially agreed to the use of their facial images as publicly available research data. Written informed consent was obtained from all research participants, and no objections were raised regarding data usage. The individually signed consent statements can be verified through the following link: [link].\n\nConflicts of Interest\n\nThe author declares that there is no conflict of interests regarding the publication of this manuscript. In addition, the ethical issues, including plagiarism, informed consent, misconduct, data fabrication and/or falsification, double publication and/or submission, and redundancies have been completely observed by the authors\n\n9- References \n\n\t[1] S. Ullah, J. Ou, Y. Xie, and W. Tian, \u201cFacial expression recognition (FER) survey: a vision, architectural elements, and future directions,\u201d PeerJ Comput. Sci., vol. 10, pp. e2024\u2013e2024, Jun. 2024, doi: 10.7717/peerj-cs.2024.\u00a0\n\n\t[2] Erlangga Satrio Agung, A. P. Rifai, and Titis Wijayanto, \u201cImage-based facial emotion recognition using convolutional neural network on emognition dataset,\u201d Scientific Reports, vol. 14, no. 1, Jun. 2024, doi: https://doi.org/10.1038/s41598-024-65276-x.\u00a0\n\n\t[3] \u201cFERFM: An Enhanced Facial Emotion Recognition System Using Fine-tuned MobileNetV2 Architecture,\u201d IETE Journal of Research, 2024, doi: https://doi.org/10.1080//03772063.2023.2202158.\u00a0\n\n\t[4] C. Scarpazza et al., \u201cThe Emotion Authenticity Recognition (EAR) test: normative data of an innovative test using dynamic emotional stimuli to evaluate the ability to recognize the authenticity of emotions expressed by faces,\u201d Neurological Sciences, Jul. 2024, doi: https://doi.org/10.1007/s10072-024-07689-0.\u00a0\n\n\t[5] M. J. Hasan, N. Mohammed, S. Rahman, and P. Koehn, \u201cHadaSmileNet: Hadamard fusion of handcrafted and deep-learning features for enhancing facial emotion recognition of genuine smiles,\u201d arXiv.org, 2025. https://arxiv.org/abs/2509.18550 (accessed Sep. 28, 2025).\u00a0\n\n\t[6] \u201cEmotion-Based Human-Computer Interaction,\u201d IGI Global eBooks, 2022, pp. 136\u2013150, doi: 10.4018/978-1-6684-5673-6.ch009.\u00a0\n\n\t[7] S. Jia, S. Wang, C. Hu, P. J. Webster, and X. Li, \u201cDetection of Genuine and Posed Facial Expressions of Emotion: Databases and Methods,\u201d Frontiers in Psychology, vol. 11, Jan. 2021, doi: 10.3389/fpsyg.2020.580287.\u00a0\n\n\t[8] D. Bhagat, A. Vakil, R. K. Gupta, and A. Kumar, \u201cFacial Emotion Recognition (FER) using Convolutional Neural Network (CNN),\u201d Procedia Computer Science, vol. 235, pp. 2079\u20132089, Jan.", "mimetype": "text/plain", "start_char_idx": 65752, "end_char_idx": 69444, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "7ad8d4be-c2e0-44d7-aed5-575ec7d380f8": {"__data__": {"id_": "7ad8d4be-c2e0-44d7-aed5-575ec7d380f8", "embedding": null, "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37", "node_type": "4", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "ff9b494f06ab2d9301cf95741443fa088208aac93ccbbf897ba21c90e3f8ad47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d71f7c2-63ad-46c0-a68b-86a68fca7f25", "node_type": "1", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "c3097cc25d65741e98c5b024a845d4f9ec1345dac5822fd16ba6f209a26944a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8b7adff-aa90-471e-9f23-82d6ce6372ec", "node_type": "1", "metadata": {}, "hash": "f44f371c45fc42e1b6e5ecf65114fec95265d8a47d5b6932b0e3412927ea821c", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "136\u2013150, doi: 10.4018/978-1-6684-5673-6.ch009.\u00a0\n\n\t[7] S. Jia, S. Wang, C. Hu, P. J. Webster, and X. Li, \u201cDetection of Genuine and Posed Facial Expressions of Emotion: Databases and Methods,\u201d Frontiers in Psychology, vol. 11, Jan. 2021, doi: 10.3389/fpsyg.2020.580287.\u00a0\n\n\t[8] D. Bhagat, A. Vakil, R. K. Gupta, and A. Kumar, \u201cFacial Emotion Recognition (FER) using Convolutional Neural Network (CNN),\u201d Procedia Computer Science, vol. 235, pp. 2079\u20132089, Jan. 2024, doi: 10.1016/j.procs.2024.04.197.\u00a0\n\n\t[9] J. A. Ballesteros, G. M. Ram\u00edrez, F. Moreira, A. Solano, and C. A. Pelaez, \u201cFacial emotion recognition through artificial intelligence,\u201d Frontiers in Computer Science, vol. 6, Jan. 2024, doi: 10.3389/fcomp.2024.1359471.\u00a0\n\n\t[10] R. Mathur and V. Gupta, \u201cEmotion detection from facial images: A hybrid approach to feature extraction and classification,\u201d World Journal of Advanced Research and Reviews, vol. 24, no. 2, pp. 2227\u20132234, Nov. 2024, doi: 10.30574/wjarr.2024.24.2.3620.\u00a0\n\n\t[11] M. Anand and S. Babu, \u201cMulti-class Facial Emotion Expression Identification Using DL-Based Feature Extraction with Classification Models,\u201d International Journal of Computational Intelligence Systems, vol. 17, no. 1, Feb. 2024, doi: 10.1007/s44196-024-00406-x.\u00a0\n\n\t[12] P. Manimohan, C. S. Keerthi, S. K. Sudha, D. M. C. Reddy, C. Mahendra, and R. Nagarjuna, \u201cHuman Emotion Detection Using CNN and Transfer Learning,\u201d International Research Journal on Advanced Engineering and Management (IRJAEM), vol. 2, no. 5, pp. 1365\u20131371, May 2024, doi: 10.47392/IRJAEM.2024.0188.\u00a0\n\n\t[13] A. Khuntia and S. Kale, \u201cReal Time Emotion Analysis Using Deep Learning for Education, Entertainment, and Beyond,\u201d arXiv preprint, Jul. 2024, arXiv:2407.04560.\u00a0\n\n\t[14] U. Islam, R. Mahum, A. AlSalman, M. Sharaf, H. Hassan, and B. Huang, \u201cFacial Emotions Detection using an Efficient Neural Architecture Search Network,\u201d Preprints, Sep. 2023, doi: 10.20944/preprints202309.1273.v1.\u00a0\n\n\t[15] I. Haider, H.-J. Yang, G.-S. Lee, and S.-H. Kim, \u201cRobust Human Face Emotion Classification Using Triplet-Loss-Based Deep CNN Features and SVM,\u201d Sensors, vol. 23, no. 10, p. 4770, May 2023, doi: 10.3390/s23104770.\u00a0\n\n\t[16] A. Singh, \u201cRealtime Facial Emotion Detection,\u201d International Journal for Research in Applied Science and Engineering Technology (IJRASET), vol. 12, no. 3, pp. 3226\u20133229, Mar. 2024, doi: 10.22214/ijraset.2024.59630.\u00a0\n\n\t[17] J. Parel et al., \u201cReal-Time Facial Emotion Detection Application with Image Processing Based on Convolutional Neural Network (CNN),\u201d International Journal of Electrical Engineering, Mathematics and Computer Science (IJEEMCS), vol. 1, no. 4, pp. 27\u201336, Nov. 2024, doi: 10.62951/ijeemcs.v1i4.123.", "mimetype": "text/plain", "start_char_idx": 68988, "end_char_idx": 71681, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "c8b7adff-aa90-471e-9f23-82d6ce6372ec": {"__data__": {"id_": "c8b7adff-aa90-471e-9f23-82d6ce6372ec", "embedding": null, "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37", "node_type": "4", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "ff9b494f06ab2d9301cf95741443fa088208aac93ccbbf897ba21c90e3f8ad47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7ad8d4be-c2e0-44d7-aed5-575ec7d380f8", "node_type": "1", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "414d942e826490651b175705b67e4268ac57a9a30e2c02372d8d36be72f7c77f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3dfc22e8-725e-4b1a-a9fa-8a89553818da", "node_type": "1", "metadata": {}, "hash": "9ee26bbcd9a0e790c4a9683ea5e95e3787b78a745224b2ed045e78551292ee90", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "10, p. 4770, May 2023, doi: 10.3390/s23104770.\u00a0\n\n\t[16] A. Singh, \u201cRealtime Facial Emotion Detection,\u201d International Journal for Research in Applied Science and Engineering Technology (IJRASET), vol. 12, no. 3, pp. 3226\u20133229, Mar. 2024, doi: 10.22214/ijraset.2024.59630.\u00a0\n\n\t[17] J. Parel et al., \u201cReal-Time Facial Emotion Detection Application with Image Processing Based on Convolutional Neural Network (CNN),\u201d International Journal of Electrical Engineering, Mathematics and Computer Science (IJEEMCS), vol. 1, no. 4, pp. 27\u201336, Nov. 2024, doi: 10.62951/ijeemcs.v1i4.123.\u00a0\n\n\t[18] A. Ashraf, T. S. Gunawan, F. Arifin, M. Kartiwi, A. Sophian, and M. H. Habaebi, \u201cEnhanced Emotion Recognition in Videos: A Convolutional Neural Network Strategy for Human Facial Expression Detection and Classification,\u201d Indonesian Journal of Electrical Engineering and Informatics (IJEEI), vol. 11, no. 1, Mar. 2023, doi: 10.52549/ijeei.v11i1.4449.\u00a0\n\n\t[19] D. Pruthviraja, U. M. Kumar, S. Parameswaran, V. G. Chowdary, and V. Bharadwaj, \u201cDeep convolutional neural network architecture for facial emotion recognition,\u201d PeerJ Computer Science, vol. 10, p. e2339, Dec. 2024, doi: 10.7717/peerj-cs.2339.\u00a0\n\n\t[20] M. Cardaioli et al., \u201cFace the Truth: Interpretable Emotion Genuineness Detection,\u201d IEEE International Joint Conference on Neural Networks (IJCNN), pp. 1\u20138, Jul. 2022, doi: 10.1109/IJCNN55064.2022.\u00a0\n\n\t[21] A. Miolla, M. Cardaioli, and C. Scarpazza, \u201cPadova Emotional Dataset of Facial Expressions (PEDFE): A unique dataset of genuine and posed emotional facial expressions,\u201d Behavior Research Methods, Aug. 2022, doi: 10.3758/s13428-022-01914-4.\u00a0\n\n\t[22] S. Annadurai, M. Arock, and A. Vadivel, \u201cReal and fake emotion detection using enhanced boosted support vector machine algorithm,\u201d Multimedia Tools and Applications, Jun. 2022, doi: 10.1007/s11042-022-13210-6.\u00a0\n\n\t[23] M. P. Sunil, S. A. Hariprasad, S. Shrishti, and S. Sriharshini, \u201cDiscrimination Between Fake and Real Emotion Using Modified CNN Model,\u201d Lecture Notes in Networks and Systems, pp. 407\u2013416, Jan. 2023, doi: 10.1007/978-981-19-9304-6_38.\u00a0\n\n\t[24] E. E. Arslan, M. F. Ak\u015fahin, M. Yilmaz, and H. E. Ilg\u0131n, \u201cTowards Emotionally Intelligent Virtual Environments: Classifying Emotions through a Biosignal-Based Approach,\u201d Applied Sciences, vol. 14, no. 19, p. 8769, Sep. 2024, doi: 10.3390/app14198769.\u00a0\n\n\t[25] J. Jia, H. Zhang, and J. Liang, \u201cBridging Discrete and Continuous: A Multimodal Strategy for Complex Emotion Detection,\u201d arXiv preprint, Sep. 2024, arXiv:2409.07901.\u00a0\n\n\t[26] J. Govea, A. M. Navarro, S. S\u00e1nchez-Viteri, and W. Villegas-Ch, \u201cImplementation of Deep Reinforcement Learning Models for Emotion Detection and Personalization of Learning in Hybrid Educational Environments,\u201d Frontiers in Artificial Intelligence, vol. 7, Nov. 2024, doi: 10.3389/frai.2024.1458230.", "mimetype": "text/plain", "start_char_idx": 71109, "end_char_idx": 73943, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}, "3dfc22e8-725e-4b1a-a9fa-8a89553818da": {"__data__": {"id_": "3dfc22e8-725e-4b1a-a9fa-8a89553818da", "embedding": null, "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e1e21035-ad3e-4f3c-ab8d-b8b199657f37", "node_type": "4", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "ff9b494f06ab2d9301cf95741443fa088208aac93ccbbf897ba21c90e3f8ad47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8b7adff-aa90-471e-9f23-82d6ce6372ec", "node_type": "1", "metadata": {"file_name": "Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_path": "A:\\game\\Research Help\\uploaded_projects\\a840e7b2-9b62-47b2-8272-221687953c44\\Final Paper - ESJ (GEN VS FAKE) without track.docx", "file_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document", "file_size": 3642677, "creation_date": "2026-02-11", "last_modified_date": "2026-02-11"}, "hash": "ce64d0c93c313d4b4985ebbd3b69865c5e89f2a76622f2dfd2bd7d4d63dc5cec", "class_name": "RelatedNodeInfo"}}, "metadata_template": "{key}: {value}", "metadata_separator": "\n", "text": "14, no. 19, p. 8769, Sep. 2024, doi: 10.3390/app14198769.\u00a0\n\n\t[25] J. Jia, H. Zhang, and J. Liang, \u201cBridging Discrete and Continuous: A Multimodal Strategy for Complex Emotion Detection,\u201d arXiv preprint, Sep. 2024, arXiv:2409.07901.\u00a0\n\n\t[26] J. Govea, A. M. Navarro, S. S\u00e1nchez-Viteri, and W. Villegas-Ch, \u201cImplementation of Deep Reinforcement Learning Models for Emotion Detection and Personalization of Learning in Hybrid Educational Environments,\u201d Frontiers in Artificial Intelligence, vol. 7, Nov. 2024, doi: 10.3389/frai.2024.1458230.\u00a0\n\n\t[27] D. Evangeline and A. Parkavi, \u201cFacial Emotion Recognition of Online Learners Using a Hybrid Deep Learning Model,\u201d International Journal of Intelligent Engineering and Systems, vol. 17, no. 6, pp. 735\u2013751, Dec. 2024, doi: 10.22266/ijies2024.1231.56.\n\n\t[28] V. Rathod et al., \u201cImproved Remote Mental Health Illness Assessment and Detection Using Facial Emotion Detection and Speech Emotion Detection,\u201d International Journal of Health Sciences, May 2022, doi: 10.53730/ijhs.v6ns2.7508.\u00a0\n\n\t[29] R. Chethan and L. Vinay, \u201cDeep Learning Based Emotion Detection System,\u201d International Journal of Scientific Research in Engineering and Management (IJSREM), vol. 8, pp. 1\u201313, Jul. 2024, doi: 10.55041/IJSREM36445.\u00a0\n\n\t[30] A. L. Barnwal and R. Barik, \u201cHuman Mood Detection using Image Processing and Machine Learning and Deep Learning,\u201d International Journal of Soft Computing and Engineering (IJSCE), Apr. 2025. [Online]. Available: https://www.ijsce.org/portfolio-item/I97000812923/\u00a0\n\n\t[31] C. Zhang, \u201cImage-based Facial Emotion Detection SYSTEM,\u201d Computer Life, vol. 12, no. 2, pp. 20\u201326, Aug. 2024, doi: 10.54097/b02twk08.\u00a0\n\n\t[32] A. Singh et al., \u201cReal-Time Emotion Recognition System Using Facial Expressions,\u201d Indian Scientific Journal of Research in Engineering and Management (IJSREM), Apr. 2024. [Online]. Available: https://ijsrem.com/download/real-time-emotion-recognition-system-using-facial-expressions/\u00a0\n\n\t[33] A. H. Ton-That and N. T. Cao, \u201cFacial Expression Recognition Using a Novel Modeling of Combined Gray Local Binary Pattern,\u201d Advances in Human-Computer Interaction, vol. 2022, pp. 1\u201312, Sep. 2022, doi: 10.1155/2022/6798208.\u00a0\n\n\t[34] Diya, Sarah Tasnim; Ferdos,  Most.Jannatul; Rahman, Md Mizanur (2025), \u201cGenuine and Fake Facial Emotion Dataset (GFFD-2025)\u201d, Mendeley Data, V1, doi: 10.17632/wmfd4p3z32.1\n\nPage | 23\n\nPage | 24\n\nPage | 1", "mimetype": "text/plain", "start_char_idx": 73406, "end_char_idx": 75802, "metadata_seperator": "\n", "text_template": "{metadata_str}\n\n{content}", "class_name": "TextNode"}, "__type__": "1"}}}