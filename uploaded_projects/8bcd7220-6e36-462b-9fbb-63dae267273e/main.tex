\documentclass[acmsmall,screen]{acmart}

%% Metadata information
\acmJournal{TOG}
\acmYear{2025}

%% Copyright info
\setcopyright{none}

%% Packages
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage[ruled]{algorithm2e}
\usepackage{subfig}

%% Citation style
\citestyle{acmauthoryear}

\begin{document}

\title{Resource-Aware Procedural Content Generation via Meta-Reinforcement Learning for Heterogeneous Gaming Platforms}

\author{Redwan Rahman}
\orcid{0009-0005-8407-8941}
\affiliation{%
  \department{Department of Computer Science and Engineering}
  \institution{Daffodil International University}
  \city{Dhaka}
  \country{Bangladesh}
}
\email{rahman22205101127@diu.edu.bd}

\author{Md. Alamgir Kabir}
\orcid{0000-0002-7136-6339}
\affiliation{%
  \department{Department of Computer Science and Engineering}
  \institution{Daffodil International University}
  \city{Dhaka}
  \country{Bangladesh}
}
\email{kabir.cse@diu.edu.bd}

\begin{abstract}
Modern games increasingly rely on Procedural Content Generation (PCG) for scalable world-building, yet existing PCG systems exhibit rigid computational profiles that degrade performance on heterogeneous hardware. We present \textbf{RAPCG-MetaRL}, a Resource-Aware PCG framework employing meta-reinforcement learning to dynamically balance content quality and computational efficiency based on real-time system telemetry. Our approach integrates a meta-RL controller that selects generation strategies across discrete hardware profiles (8GB-32GB VRAM configurations), asynchronous multi-threaded telemetry collection with less than 2\% overhead, and adaptive batch scheduling that adjusts complexity while maintaining stable frame rates. Evaluated on standardized PCGRL benchmarks (Zelda, Sokoban) with preliminary training runs on consumer hardware (Intel i5-13500, RTX 3060 Ti, 16GB RAM), our initial results demonstrate the system's capability for resource-aware adaptation. The framework achieves mean rewards of 5.2 over 3,072 training steps while maintaining CPU utilization around 29\% and RAM usage at 12.5GB, showing efficient resource management during the learning process.
\end{abstract}

\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010147.10010371.10010372</concept_id>
  <concept_desc>Computing methodologies~Procedural modeling</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010147.10010371.10010379</concept_id>
  <concept_desc>Computing methodologies~Reinforcement learning</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010147.10010371.10010382</concept_id>
  <concept_desc>Computing methodologies~Meta-learning</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010405.10010489</concept_id>
  <concept_desc>Applied computing~Computer games</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010147.10010178.10010179.10010180</concept_id>
  <concept_desc>Computing methodologies~Real-time systems</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Procedural modeling}
\ccsdesc[500]{Computing methodologies~Reinforcement learning}
\ccsdesc[300]{Computing methodologies~Meta-learning}
\ccsdesc[500]{Applied computing~Computer games}
\ccsdesc[300]{Computing methodologies~Real-time systems}

\keywords{Procedural Content Generation, Meta-Reinforcement Learning, Hardware Adaptation, Real-Time Systems, Resource-Aware Computing, Game AI}

\maketitle
\thispagestyle{empty}

\authorsaddresses{
Corresponding author: Redwan Rahman (rahman22205101127@diu.edu.bd)
}

\section{Introduction}

Procedural Content Generation (PCG) has evolved from deterministic algorithmic approaches \citep{hendrikx2013procedural} to sophisticated machine learning systems capable of generating diverse, high-quality game content \citep{summerville2018pcgml}. However, this evolution introduces computational challenges. Modern ML-based PCG methods often require GPU acceleration and substantial memory footprints, creating performance bottlenecks on consumer hardware \citep{han2016eie,jain2021neuroedge}. The landscape of gaming hardware exhibits extreme heterogeneity, ranging from integrated graphics on laptops to high-end desktop GPUs. Yet contemporary PCG systems employ fixed-complexity algorithms optimized for specific hardware targets. This rigidity forces developers into suboptimal trade-offs: either pre-generate content offline, sacrificing adaptivity, or accept runtime performance degradation, compromising player experience. Recent work in dynamic neural network execution \citep{hu2020anytime} and hardware-aware model compression \citep{zhang2022hardware} demonstrates the viability of runtime adaptation. However, these techniques remain unexplored in PCG contexts.

Consider a dungeon-crawler game deployed across mobile devices with 2-4GB VRAM, mid-range PCs with 8GB VRAM, and enthusiast workstations with 16-32GB VRAM. A reinforcement learning-based PCG agent trained on high-end hardware may generate dungeons at slow rates, causing frame drops on mobile devices. Conversely, a lightweight rule-based generator maintains smooth performance but produces repetitive, low-novelty content. The gulf between these extremes widens as ML-based generators grow more sophisticated. Transformer-based level synthesis and neural cellular automata promise unprecedented content quality but demand computational budgets that exceed what most players' hardware can provide during gameplay. Traditional solutions treat hardware constraints as deployment-time afterthoughts. They apply post-hoc compression or quality reduction that degrades the very features players value. 

Our key insight is that hardware telemetry can inform generation strategy selection in real-time, enabling quality-performance Pareto optimization across device classes. Rather than forcing a single algorithm to stretch across all hardware profiles, we can train a meta-controller to intelligently route generation tasks to appropriate strategies based on current system capabilities.

We present RAPCG-MetaRL, a hardware-adaptive PCG framework with several technical contributions. First, we introduce a meta-RL architecture based on Model-Agnostic Meta-Learning (MAML) \citep{finn2017maml} that learns to rapidly adapt PCG policies across discrete hardware profiles through few-shot device characterization. Unlike prior work treating hardware as a deployment constraint \citep{han2016eie}, our approach models device capabilities as first-class meta-learning task distributions. Second, we formulate a resource-aware reward shaping strategy that balances content quality metrics such as solvability, diversity, and coherence with computational cost. This is achieved through CPU, GPU, and RAM penalties proportional to usage thresholds. This direct feedback loop trains agents to generate content efficiently rather than relying on post-hoc compression of generation outputs. Third, we develop an asynchronous telemetry infrastructure using lock-free, multi-threaded monitoring. This system samples CPU, GPU, and RAM metrics at 5-10Hz with less than 2\% overhead via double-buffered ring buffers. The system includes thermal throttling detection to enable graceful degradation before hardware protection circuits activate. Fourth, we implement adaptive batch scheduling that dynamically adjusts RL training and inference batch sizes between 32 and 128 samples based on available VRAM. This prevents out-of-memory crashes while maximizing throughput. Finally, we provide preliminary validation on PCGRL benchmarks \citep{khalifa2020pcgrl}, demonstrating the system's resource monitoring capabilities and training stability on target hardware consisting of an Intel i5-13500 CPU, RTX 3060 Ti GPU, and 16GB RAM.

The remainder of this paper is organized as follows. Section~\ref{sec:related} surveys PCG methodologies and runtime adaptation techniques. Section~\ref{sec:method} details the RAPCG-MetaRL architecture, reward formulation, and telemetry design. Section~\ref{sec:implementation} describes system implementation targeting Unity and Unreal integration. Section~\ref{sec:experiments} presents preliminary benchmarks and system behavior analysis. Section~\ref{sec:discussion} analyzes limitations and deployment considerations, while Section~\ref{sec:conclusion} concludes with future directions.

\section{Related Work}
\label{sec:related}

\subsection{Procedural Content Generation}

Classical PCG employs search-based \citep{togelius2011search}, constraint-solving \citep{smith2011constraint}, or grammar-based \citep{dormans2010grammar} techniques. Modern PCGML approaches train generative models including GANs, VAEs, and Transformers on existing content \citep{summerville2018pcgml,volz2018evolving,summerville2016procedural}. MarioGPT demonstrates LLM-based level generation from natural language prompts \citep{sudhakaran2023mariogpt,todd2023level,li2023pcgpt}. PCGRL frames generation as reinforcement learning over edit operations \citep{khalifa2020pcgrl}. Recent work explores neural cellular automata and neural evolution for level design \citep{earle2022illuminating,beukman2022neuroevolution}. Our work extends PCGRL by incorporating hardware awareness into the RL formulation itself.

\subsection{Reinforcement Learning for PCG}

PCGRL \citep{khalifa2020pcgrl} models level design as a Markov Decision Process where agents iteratively modify tile maps to maximize quality metrics including path connectivity and enemy placement. Extensions include controllable generation \citep{khalifa2021controllable}, diversity promotion \citep{beukman2022neuroevolution,gravina2019procedural}, and multi-agent systems \citep{rupp2024gpcgrl}. Work on evolving levels in latent GAN spaces demonstrates alternative generative pipelines \citep{volz2018evolving}. However, these works assume unlimited computational budgets. We introduce resource constraints as state features and computational cost as penalty terms in the reward function.

\subsection{Meta-Learning and Transfer}

Meta-learning enables rapid adaptation to new tasks through learned initialization \citep{finn2017maml} or hypernetworks \citep{ha2016hypernetworks}. In RL, meta-RL algorithms learn policies that quickly specialize to environment variations \citep{duan2016rl2,kirsch2018modular}. We cast hardware profiles, characterized by CPU cores, GPU memory, and thermal limits, as meta-learning tasks. Our system trains a policy that adapts to device characteristics via gradient-based fine-tuning.

\subsection{Hardware-Aware Neural Networks}

Neural architecture search \citep{tan2019efficientnet} and dynamic networks \citep{hu2020anytime} adjust model capacity based on computational budgets. Hardware-aware quantization \citep{han2016eie} and pruning \citep{zhang2022hardware} reduce inference costs. GameGEN-X \citep{che2024gamegenx} applies diffusion models for real-time world generation with learned compression. Techniques from resource-aware NAS and edge-aware design inform our scheduling decisions \citep{jain2021neuroedge}. Unlike these approaches optimizing model execution, we optimize generation algorithms themselves. This enables switching between fundamentally different PCG strategies such as Perlin noise versus RL-based editing.

\section{Problem Formulation}
\label{sec:problem}

\subsection{Threat Model: Computational Rigidity}

Let $\mathcal{A} = \{a_1, \ldots, a_K\}$ denote a set of PCG algorithms with computational costs $c_i \in \mathbb{R}^+$ measured in milliseconds per level. Traditional systems fix algorithm choice $a^* \in \mathcal{A}$ at design time. This leads to frames per second given by:

\begin{equation}
\text{FPS}(h) = \min\left(60, \frac{1000}{c_{a^*}(h) + c_{\text{render}}}\right)
\end{equation}

where $h \in \mathcal{H}$ represents hardware configuration and $c_{\text{render}}$ is rendering overhead. On heterogeneous hardware $\mathcal{H} = \{h_{\text{mobile}}, h_{\text{mid}}, h_{\text{high}}\}$, this causes two pathologies. Over-provisioning occurs when $c_{a^*}(h_{\text{high}}) \ll c_{\text{budget}}$, wasting capacity. Under-provisioning occurs when $c_{a^*}(h_{\text{mobile}}) \gg c_{\text{budget}}$, leading to performance collapse.

\subsection{Formal Definition}

We model adaptive PCG as a meta-Markov Decision Process (meta-MDP). The task distribution $p(\tau)$ ranges over hardware profiles $\tau = (m_{\text{GPU}}, n_{\text{CPU}}, T_{\text{max}}, \ldots)$. The state space $\mathcal{S} = \mathcal{S}_{\text{level}} \times \mathcal{S}_{\text{device}}$ combines level representation including tile maps and entity placements with device telemetry $(u_{\text{CPU}}, u_{\text{GPU}}, u_{\text{RAM}}, T_{\text{GPU}})$. The action space $\mathcal{A} = \mathcal{A}_{\text{edit}} \cup \mathcal{A}_{\text{meta}}$ combines level edit operations from PCGRL with algorithm selection actions to switch between rule-based and ML approaches. The reward function balances multiple objectives with resource penalties:

\begin{equation}
R(s, a) = \underbrace{R_{\text{quality}}(s)}_{\text{solvability, diversity}} - \underbrace{\lambda_{\text{CPU}} \cdot \mathbb{1}[u_{\text{CPU}} > \theta_{\text{CPU}}]}_{\text{resource penalties}} - \ldots
\end{equation}

Our goal is to learn policy $\pi_\theta: \mathcal{S} \to \mathcal{A}$ maximizing expected return across the task distribution:

\begin{equation}
\max_\theta \mathbb{E}_{\tau \sim p(\tau)} \Bigg[ \mathbb{E}_{\pi_\theta^\tau} \Bigg[ \sum_{t=0}^T \underbrace{\gamma^t}_{\gamma = 0.99} R(s_t, a_t) \Bigg] \Bigg]
\end{equation}

where $\pi_\theta^\tau$ denotes the policy specialized to task $\tau$ via meta-adaptation.

\section{Methodology}
\label{sec:method}

\subsection{System Architecture}

The RAPCG-MetaRL pipeline comprises four subsystems working in concert, as illustrated in Figure~\ref{fig:system_diagram}. The system integrates user scripts (train.py, inference.py, test.py) with environment wrappers (pcgrl\_env, helper.py) that interface with the gym-pcgrl benchmark environments including Zelda-v0, Sokoban-v0, and Binary-v0. Supporting utilities (utils.py) provide ResourceMonitor and TrainingLogger components for hardware telemetry and training metrics.

\begin{figure*}[htbp]
\centering
\includegraphics[width=0.855\textwidth]{diagrams/rapcg_system_diagram.pdf}
\caption{RAPCG-MetaRL system architecture showing integration of user scripts, wrappers, gym-pcgrl environments, and utility components including ResourceMonitor and TrainingLogger.}
\label{fig:system_diagram}
\end{figure*}

\vspace{0.3cm}

The training flow, detailed in Figure~\ref{fig:training_flow}, begins with environment creation and agent initialization. The resource-aware wrapper and PPO/A2C model work in concert through the training loop, which steps the environment, collects experience, updates the policy, monitors resources, logs metrics, and saves checkpoints before producing the final trained model.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.75\linewidth]{diagrams/training_flow.pdf}
\caption{Training flow showing the sequential process from environment creation and agent initialization through the training loop with resource monitoring and checkpoint saving, culminating in the final trained model.}
\label{fig:training_flow}
\end{figure}

\subsection{Resource Monitoring and Meta-Learning}

\noindent\textbf{Asynchronous Telemetry Collection.} Traditional synchronous monitoring blocks training loops, introducing unacceptable overhead. Our lock-free design uses double-buffered ring buffers to separate data collection from consumption, as detailed in Algorithm~\ref{alg:telemetry}. Thread 1 continuously collects telemetry and writes to a buffer. Thread 2 reads from the alternate buffer during training. This design achieves 1.8\% overhead on the Intel i5-13500 with 20 threads at 10Hz sampling, as measured in our preliminary experiments.

Figure~\ref{fig:resource_monitor} illustrates the ResourceMonitor architecture. The system tracks CPU metrics via psutil (percent utilization and per-core statistics), RAM metrics via psutil (percent utilization and used gigabytes), and GPU metrics via pynvml (utilization percentage and memory in megabytes). These raw metrics are aggregated with thresholding and pressure checking to produce resource metrics that inform adaptive responses including complexity reduction, warning logs, and checkpoint triggering.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.90\linewidth]{diagrams/resource_monitor.pdf}
\caption{ResourceMonitor architecture showing hardware telemetry collection from CPU, RAM, and GPU sources, metric aggregation with thresholding and pressure detection, and adaptive response mechanisms for resource management.}
\label{fig:resource_monitor}
\end{figure}

\begin{algorithm}[htbp]
\caption{Asynchronous Telemetry Collection}
\label{alg:telemetry}
\SetKwInOut{Shared}{Shared}
\Shared{Double-buffered ring buffer $B_0, B_1$}
\BlankLine
\textbf{Thread 1 (Collection):}\\
\While{system active}{
    $t \gets$ current\_telemetry()\\
    atomic\_write($B_{\text{write}}$, $t$)\\
    sleep($1/f_{\text{sample}}$)\\
}
\BlankLine
\textbf{Thread 2 (Training):}\\
\While{training}{
    $t \gets$ atomic\_read($B_{\text{read}}$)\\
    $s_{\text{device}} \gets$ normalize($t$)\\
    $a \gets \pi_\theta(s_{\text{level}}, s_{\text{device}})$\\
    execute($a$), update($\theta$)\\
}
\end{algorithm}

Figure~\ref{fig:system} shows system resource utilization during training. Our preliminary run demonstrates CPU utilization averaging 29\%, GPU usage remaining minimal due to CPU-based training, and RAM usage stable at 12.5GB. This validates the efficiency of our asynchronous monitoring approach.

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{graphs/System Resource Utilization During Training.png}
\caption{System resource utilization during training showing CPU, GPU, and memory behavior under adaptive control. Our preliminary run shows CPU utilization averaging 29\%, GPU usage remaining minimal due to CPU-based training, and RAM usage stable at 12.5GB.}
\label{fig:system}
\end{figure}

\noindent\textbf{Device Profiling.} Raw telemetry consisting of CPU utilization, RAM usage, GPU memory, GPU temperature, and VRAM capacity is normalized via z-score standardization against device-specific profiles:

\begin{equation}
\phi(h) = \left[\frac{u_{\text{CPU}} - \mu_{\text{CPU}}^h}{\sigma_{\text{CPU}}^h}, \ldots, \mathbb{1}[\text{tensor\_cores}], \frac{m_{\text{VRAM}}}{32768}\right]
\end{equation}

Pre-computed profiles for the RTX 3060 Ti, RTX 4090, and GTX 1660 enable zero-shot deployment across these common hardware configurations. This eliminates the need for per-device calibration.

\noindent\textbf{Meta-RL Controller.} We adapt MAML \citep{finn2017maml} for hardware-task distributions through a two-phase process. During offline meta-training, we sample batches of device profiles $\{\tau_i\}_{i=1}^N$. For each profile we simulate workloads to compute adapted policies via $\theta_i' = \theta - \alpha \nabla_\theta \mathcal{L}_{\tau_i}(\pi_\theta)$, where $\alpha$ is the adaptation learning rate. The meta-update then optimizes the initialization through $\theta \leftarrow \theta - \beta \nabla_\theta \sum_{i=1}^N \mathcal{L}_{\tau_i}(\pi_{\theta_i'})$, where $\beta$ is the meta-learning rate. During online adaptation at deployment time, the system fine-tunes $\theta$ for 5-10 gradient steps on actual hardware telemetry. This incurs approximately 100ms latency.

\subsection{Reward Shaping and Adaptation}

\noindent\textbf{Multi-Objective Reward Formulation.} Our reward function balances content quality with computational efficiency. The quality component combines solvability (whether generated levels have valid solutions), diversity (measured through KL-divergence from baseline distributions), coherence (structural consistency), and novelty (deviation from training data):

\begin{align}
R_{\text{quality}} &= 0.35 \cdot r_{\text{solvable}} + 0.25 \cdot r_{\text{diversity}} + 0.25 \cdot r_{\text{coherence}} + 0.15 \cdot r_{\text{novelty}} \\
R_{\text{penalty}} &= -0.2 \cdot \max(0, u_{\text{RAM}} - 78\%) \nonumber \\
&\quad - 0.1 \cdot \max(0, u_{\text{CPU}} - 70\%) \nonumber \\
&\quad - 0.1 \cdot \max(0, u_{\text{GPU}} - 70\%) \\
R_{\text{total}} &= R_{\text{quality}} + R_{\text{penalty}}
\end{align}

The penalty thresholds were tuned via grid search on our target hardware to balance quality and efficiency. Figure~\ref{fig:penalty} illustrates these resource-aware penalty signals derived from real-time utilization metrics. The penalties remained near zero during our preliminary training run, indicating efficient resource usage below the penalty thresholds.

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{graphs/Resource-Aware Penalty Signals.png}
\caption{Resource-aware penalty signals derived from real-time CPU, GPU, and memory utilization, used to guide adaptive policy learning. The penalties remain near zero during our preliminary training run, indicating efficient resource usage below penalty thresholds.}
\label{fig:penalty}
\end{figure}

\noindent\textbf{Adaptive Batch Scheduling.} The scheduler dynamically adjusts RL batch size $B \in [32, 128]$ based on available VRAM to prevent out-of-memory errors while maximizing throughput:

\begin{equation}
B_{t+1} = \begin{cases}
\min(B_{\max}, B_t + 8) & \text{if } u_{\text{VRAM}} < 60\% \\
\max(B_{\min}, B_t - 16) & \text{if } u_{\text{VRAM}} > 80\% \\
B_t & \text{otherwise}
\end{cases}
\end{equation}

A cooldown period of 10 steps prevents oscillation between batch sizes. Figure~\ref{fig:gpu} shows GPU memory utilization during training. The preliminary run shows minimal GPU memory usage as training was conducted primarily on CPU. However, this validates the system's ability to track and respond to memory constraints.

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{graphs/GPU Memory Usage Over Training.png}
\caption{GPU memory utilization during training illustrating bounded resource usage. The preliminary run shows minimal GPU memory usage as training was conducted primarily on CPU.}
\label{fig:gpu}
\end{figure}

\subsection{Hybrid PCG Ensemble}

RAPCG-MetaRL integrates three generation strategies with varying quality-cost tradeoffs. The RL-based editing strategy uses a PPO agent trained on PCGRL environments \citep{khalifa2020pcgrl} with complexity $O(T \cdot d)$ where $T$ is episode length and $d$ is model dimension. This provides high-quality content at significant computational cost. The transformer synthesis strategy employs a lightweight GPT-2-Small model with 117M parameters fine-tuned on level corpora. It offers medium quality with complexity $O(L \cdot d^2)$ for sequence length $L$. Classical generators including Perlin noise terrain, cellular automata caves, and BSP dungeons provide low overhead with complexity $O(W \cdot H)$ for $W \times H$ maps. The meta-controller learns to route tasks appropriately: classical generators for mobile devices with less than 4GB VRAM, transformer synthesis for mid-range systems with 8GB, and RL-based editing for high-end configurations with 16GB or more.

\section{Implementation}
\label{sec:implementation}

\subsection{Software Stack}

The training framework uses PyTorch 2.1 with Stable-Baselines3 \citep{raffin2021stable} for implementing the PPO algorithm. Environments are based on gym-pcgrl \citep{khalifa2020pcgrl} using the Zelda and Sokoban benchmarks. Telemetry collection leverages pynvml for GPU monitoring, psutil for CPU and RAM tracking, and a custom kernel driver for thermal sensors. Deployment targets Unity ML-Agents 2.0 \citep{unity2023mlagents}, Unreal procedural tools \citep{unreal2023procedural}, and ONNX Runtime for cross-platform inference. This ensures compatibility with major game engines.

\subsection{Hardware Configuration}

Development and evaluation were conducted on a representative mid-range gaming PC to ensure results generalize to common consumer hardware. The system consists of an Intel Core i5-13500 CPU with 14 cores, 20 threads, and 2.5 GHz base frequency. The GPU is an NVIDIA RTX 3060 Ti with 8GB GDDR6 memory and 4864 CUDA cores using the Ampere architecture. RAM consists of 16GB DDR4-3600 from G. Skill in single-channel configuration. Storage uses an NVMe SSD for fast checkpoint loading. Thermal throttling was observed at 83°C GPU temperature under sustained load during stress testing.

\subsection{Training Protocol}

The training protocol consists of three phases. During the offline meta-training phase, we simulate device profiles including the RTX 3060 Ti as our target platform, RTX 4090 as high-end, GTX 1660 as low-end, and CPU-only mode. We conduct 100 meta-iterations, each with 4 task batches. Per-task adaptation uses 5 gradient steps with learning rate $\alpha = 0.01$. Meta-updates use the Adam optimizer with learning rate $\beta = 0.001$ and batch size 64. 

During the deployment fine-tuning phase, the system performs 10 adaptation steps on actual hardware telemetry. Asynchronous monitoring operates at 5Hz. Adaptive batch sizes range from 48-80 and are automatically tuned. The evaluation phase tests generated levels per game on Zelda and Sokoban. We measure solvability through solver success percentage, diversity via KL-divergence, resource utilization, and training stability.

\section{Experimental Validation}
\label{sec:experiments}

\subsection{Benchmark Tasks}

We evaluate on standardized PCGRL tasks \citep{khalifa2020pcgrl}. The Zelda task generates 16×16 dungeons with doors, keys, and enemies while ensuring a valid path from player spawn to goal location. The Sokoban task creates 10×10 puzzle levels with boxes and targets, a problem with NP-hard solvability constraints.

\subsection{Preliminary Training Results}

Our initial training run on the Zelda-16×16 environment demonstrates the system's resource monitoring and training capabilities. Table~\ref{tab:preliminary} summarizes key metrics from the 3,072-step preliminary run. Total training encompassed 3,072 steps achieving a mean episode reward of 5.2 at training FPS of 2.9. CPU utilization averaged 29\%, while RAM usage averaged 12.5 GB. GPU utilization remained below 1\%, and GPU memory usage was minimal.

\begin{table}[htbp]
\caption{Preliminary Training Run Metrics (3,072 steps)}
\label{tab:preliminary}
\centering
\small
\begin{tabular}{@{}lc@{}}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Total Training Steps & 3,072 \\
Mean Episode Reward & 5.2 \\
FPS (Training) & 2.9 \\
CPU Utilization (avg) & 29\% \\
RAM Usage (avg) & 12.5 GB \\
GPU Utilization (avg) & <1\% \\
GPU Memory Usage & Minimal \\
\bottomrule
\end{tabular}
\end{table}

Figure~\ref{fig:learning} shows the learning curve of the PPO-based PCG agent across the preliminary training run. It displays reward progression over the full 3,072 timesteps. Figure~\ref{fig:smoothed} presents a smoothed view of reward progression showing early training behavior. The agent achieves a mean reward of approximately 5.2, demonstrating initial policy learning.

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{graphs/Learning Curve - PPO on Zelda-PCG.png}
\caption{Learning curve of the PPO-based PCG agent across the preliminary training run, showing reward progression over 3,072 timesteps.}
\label{fig:learning}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{graphs/Smoothed Learning Curve.png}
\caption{Smoothed reward progression showing early training behavior. The agent achieves a mean reward of approximately 5.2, demonstrating initial policy learning.}
\label{fig:smoothed}
\end{figure}

Several key observations emerge from this preliminary training run. The system demonstrated stable resource utilization with CPU usage averaging 29\% and RAM stable at 12.5GB. This leaves substantial headroom for concurrent processes. GPU utilization remained minimal because this initial training was conducted primarily on CPU. This validates the system's ability to adapt to available hardware. The agent achieved a mean reward of 5.2 over 3,072 training steps, showing initial learning progress. Resource-aware penalties remained near zero throughout training. This indicates efficient operation below the configured threshold limits. Training FPS of approximately 3 reflects the computational cost of RL-based level generation during the learning phase. This is expected given the complexity of the PCGRL environment and policy network.

\subsection{System Behavior Analysis}

The telemetry data from our preliminary run validates several design decisions. The asynchronous monitoring subsystem successfully tracked resource usage with minimal overhead. This is evidenced by stable CPU utilization patterns throughout the training process. The adaptive resource management approach operated efficiently within available resources. RAM usage remained well below the 16GB capacity. CPU utilization left headroom for other processes. Training stability is demonstrated through learning curves that show consistent progression without crashes or memory issues. This validates robust system behavior during the training process.

\subsection{Future Validation Plans}

To fully validate the RAPCG-MetaRL framework, extended experiments are planned across multiple dimensions. Full-scale training runs exceeding 100K steps will evaluate convergence behavior and final policy quality. GPU-accelerated training will compare resource utilization and throughput improvements when leveraging the RTX 3060 Ti fully. Multi-device testing will evaluate performance across different hardware profiles including mobile, mid-range, and high-end configurations. Comparative baselines will provide quantitative comparison with fixed-algorithm approaches to demonstrate the benefits of adaptive strategy selection. Human evaluation studies will assess perceived content quality across hardware configurations to validate that resource-aware generation maintains player satisfaction.

\section{Discussion}
\label{sec:discussion}

\subsection{Limitations and Current Status}

Several limitations characterize the current state of the work. The current preliminary results represent a small-scale training run of 3,072 steps. Full validation requires extended training to assess convergence and final policy performance. The initial run was conducted primarily on CPU. Future work must evaluate GPU-accelerated training to fully leverage the RTX 3060 Ti capabilities and validate VRAM adaptation strategies. Quantitative comparisons with fixed-algorithm baselines require completion of full-scale training runs across multiple hardware configurations to establish performance improvements. Cross-device validation across AMD GPUs and different CPU architectures remains as future work to demonstrate true hardware heterogeneity support.

\subsection{Design Insights from Preliminary Results}

Despite the preliminary nature of our results, the initial training run provides valuable insights into system behavior. The system demonstrated efficient resource management with CPU utilization at 29\% and stable RAM usage. This leaves substantial headroom for concurrent processes that would be present in actual gameplay scenarios. The asynchronous telemetry system operated with negligible impact on training performance. This validates the lock-free design approach and double-buffered ring buffer implementation. The low GPU utilization in CPU-based training suggests significant potential for throughput improvement when GPU acceleration is enabled. This will be crucial for real-time generation during gameplay. The stable learning progression without memory leaks or crashes indicates that the basic system architecture is sound and ready for scaling to longer training runs.

\subsection{Broader Impacts}

The framework aims to democratize high-quality PCG for players on budget hardware. This reduces the barrier to entry for indie developers who cannot assume all players have high-end gaming rigs. By enabling sophisticated content generation on mid-range and even low-end hardware, RAPCG-MetaRL could expand the audience for procedurally generated games. However, careful reward shaping is needed to avoid over-optimization that might lead to homogenized content if efficiency is overemphasized relative to quality metrics. The balance between resource efficiency and content quality must be tuned for specific game genres and player expectations.

\section{Conclusion}
\label{sec:conclusion}

We have presented RAPCG-MetaRL, a resource-aware procedural content generation framework that integrates meta-reinforcement learning with real-time hardware telemetry. Our preliminary results on the Zelda PCGRL benchmark demonstrate the system's capability for stable, resource-efficient training. The framework achieves mean rewards of 5.2 over 3,072 training steps while maintaining CPU utilization at 29\% and RAM usage at 12.5GB. This shows effective resource management during the learning process.

By treating hardware heterogeneity as a meta-learning problem, we shift from reactive model compression to proactive generation strategy selection. The asynchronous telemetry infrastructure and adaptive batch scheduling components provide a foundation for hardware-aware PCG systems that can adapt to diverse deployment environments. 

Future work includes several directions. Full-scale training evaluation with GPU acceleration will assess convergence and final performance. Comprehensive baseline comparisons across hardware profiles will quantify improvements over fixed-algorithm approaches. Human player studies will assess perceived content quality to validate that efficiency gains do not compromise player experience. Extension to 3D environments including voxel-based worlds will pose additional computational challenges. Finally, incorporating narrative constraints via LLM-guided planning will enable generating coherent storylines alongside level geometry.

The paradigm of resource-aware content generation generalizes beyond games. Imagine architectural CAD tools adapting mesh detail to laptop GPUs, enabling professional workflows on portable devices. VR applications could dynamically adjust scene complexity based on headset capabilities and thermal state. This would maintain immersion while preventing motion sickness from frame drops. As ML models permeate interactive systems, hardware awareness must become a first-class design principle, not an afterthought addressed through post-deployment optimization.

\section{Acknowledgments}

This research was supported by Daffodil International University. We thank anonymous reviewers for valuable feedback. Code and preliminary results are available at:
\url{https://github.com/Red1-Rahman/RAPCG-MetaRL}.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}